[{"path":[]},{"path":"/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@rstudio.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to ODRF","title":"Contributing to ODRF","text":"outlines propose change ODRF. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to ODRF","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to ODRF","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed).","code":""},{"path":"/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to ODRF","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"liuyu-star/ODRF\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to ODRF","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to ODRF","text":"Please note ODRF project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with ODRF","title":"Getting help with ODRF","text":"Thanks using ODRF! filing issue, places explore pieces put together make process smooth possible.","code":""},{"path":"/SUPPORT.html","id":"make-a-reprex","dir":"","previous_headings":"","what":"Make a reprex","title":"Getting help with ODRF","text":"Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site.","code":""},{"path":"/SUPPORT.html","id":"where-to-ask","dir":"","previous_headings":"","what":"Where to ask?","title":"Getting help with ODRF","text":"Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed.","code":""},{"path":"/SUPPORT.html","id":"what-happens-next","dir":"","previous_headings":"","what":"What happens next?","title":"Getting help with ODRF","text":"efficient possible, development tidyverse packages tends bursty, shouldn’t worry don’t get immediate response. Typically don’t look repo sufficient quantity issues accumulates, ’s burst intense activity focus efforts. makes development efficient avoids expensive context switching problems, cost taking longer get back . process makes good reprex particularly important might multiple months initial report start working . can’t reproduce bug, can’t fix !","code":""},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"odrf","dir":"Articles","previous_headings":"","what":"ODRF","title":"Oblique Decision Random Forest for Classification and Regression","text":"ODRF implements well-known Oblique Decision Tree (ODT) ODT-based Random Forest (ODRF), uses linear combinations predictors partitioning variables traditional CART Random Forest. number modifications adopted implementation; new functions also provided.","code":""},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"overview","dir":"Articles","previous_headings":"ODRF","what":"Overview","title":"Oblique Decision Random Forest for Classification and Regression","text":"ODRF R package consists following main functions: ODT() classification regression using ODT node split linear combination predictors. ODRF() classification regression implemented ODRF ’s extension random forest based ODT() includes random forest special case. Online() online training update existing ODT ODRF using new data sets. prune() prune ODT bottom top validation data based prediction error. print(), predict() plot() base R functions base R Package class ODT ODRF. ODRF allows users define functions find projections node, essential performance forests. also provide complete comparison analysis ODT ODRF, details available vignette(“ODRF”).","code":""},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"installation","dir":"Articles","previous_headings":"ODRF","what":"Installation","title":"Oblique Decision Random Forest for Classification and Regression","text":"can install development version ODRF GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"liuyu-star/ODRF\")"},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"usage","dir":"Articles","previous_headings":"ODRF","what":"Usage","title":"Oblique Decision Random Forest for Classification and Regression","text":"show use ODRF package examples.","code":""},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"classification-and-regression-using-odt-and-odrf","dir":"Articles","previous_headings":"ODRF > Usage","what":"Classification and regression using ODT and ODRF","title":"Oblique Decision Random Forest for Classification and Regression","text":"Examples classification regression using ODRF ODT follows. following example, suppose training data available two batches. first batch used train ODT ODRF, second batch used update model online. error model update significantly smaller one batch data alone. Update existing ODT ODRF online. prune first judges whether error new data reduced applied, starting last leaf nodes. ODRF, argument ‘useOOB=TRUE’ uses ‘--bag’ pruning. Examples follows. Note , prune always improve efficiency number observers training set small build simple tree structure. Therefore, expand training set random numbers make prune effective.","code":"library(ODRF) #> Loading required package: partykit #> Loading required package: grid #> Loading required package: libcoin #> Loading required package: mvtnorm data(seeds, package = \"ODRF\") set.seed(12) train <- sample(1:209, 150) seeds_train <- data.frame(seeds[train, ]) seeds_test <- data.frame(seeds[-train, ]) forest <- ODRF(varieties_of_wheat ~ ., seeds_train, split = \"gini\",    parallel = FALSE) pred <- predict(forest, seeds_test[, -8]) (e.forest <- mean(pred != seeds_test[, 8])) #> [1] 0.01694915 data(body_fat, package = \"ODRF\") train <- sample(1:252, 200) bodyfat_train <- data.frame(body_fat[train, ]) bodyfat_test <- data.frame(body_fat[-train, ]) tree <- ODT(Density ~ ., bodyfat_train, split = 'mse') pred <- predict(tree, bodyfat_test[, -1]) (e.tree <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 4.047499e-05 set.seed(17) index <- sample(nrow(seeds_train), floor(nrow(seeds_train) / 2)) forest1 <- ODRF(varieties_of_wheat ~ ., seeds_train[index, ],   split = \"gini\", parallel = FALSE) pred <- predict(forest1, seeds_test[, -8]) (e.forest.1 <- mean(pred != seeds_test[, 8])) #> [1] 0.03389831 forest2 <- online(forest1, seeds_train[-index, -8], seeds_train[-index, 8]) pred <- predict(forest2, seeds_test[, -8]) (e.forest.online <- mean(pred != seeds_test[, 8])) #> [1] 0.03389831 index <- seq(floor(nrow(bodyfat_train) / 2)) tree1 <- ODT(Density ~ ., bodyfat_train[index, ], split = 'mse') pred <- predict(tree1, bodyfat_test[, -1]) (e.tree.1 <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 4.428549e-05 tree2 <- online(tree1, bodyfat_train[-index, -1], bodyfat_train[-index, 1]) pred <- predict(tree2, bodyfat_test[, -1]) (e.tree.online <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 4.418373e-05 set.seed(4) bodyfat_train=rbind(as.matrix(bodyfat_train),matrix(rnorm(3000*5),5*200,15)) seeds_train=rbind(as.matrix(seeds_train),matrix(rnorm(1200*5),5*150,8)) bodyfat_train[-seq(200),1]=sample(bodyfat_train[seq(200),1],5*200,   replace = TRUE) seeds_train[-seq(150),8]=sample(seeds_train[seq(150),8],5*150,   replace = TRUE) index <- sample(nrow(seeds_train), floor(nrow(seeds_train) / 2)) forest1 <- ODRF(seeds_train[index, -8], seeds_train[index, 8],   split = \"gini\", parallel = FALSE) pred <- predict(forest1, seeds_test[, -8]) (e.forest.1 <- mean(pred != seeds_test[, 8])) #> [1] 0.06779661 forest2 <- prune(forest1, seeds_train[-index, -8], seeds_train[-index, 8],    useOOB = FALSE) pred <- predict(forest2, seeds_test[, -8]) (e.forest.prune1 <- mean(pred != seeds_test[, 8])) #> [1] 0.05084746 forest3 <- prune(forest1, seeds_train[index, -8], seeds_train[index, 8]) pred <- predict(forest3, seeds_test[, -8]) (e.forest.prune2 <- mean(pred != seeds_test[, 8])) #> [1] 0.06779661 index <- sample(nrow(bodyfat_train), floor(nrow(bodyfat_train) / 2)) tree1 <- ODT(bodyfat_train[index, -1], bodyfat_train[index, 1], split = 'mse') pred <- predict(tree1, bodyfat_test[, -1]) (e.tree.1 <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 8.18606e-05 tree2 <- prune(tree1, bodyfat_train[-index, -1], bodyfat_train[-index, 1]) pred <- predict(tree2, bodyfat_test[, -1]) (e.tree.prune <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 7.581808e-05"},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"print-the-tree-structure-of-odt-and-odrf","dir":"Articles","previous_headings":"ODRF > Usage","what":"print the tree structure of ODT and ODRF","title":"Oblique Decision Random Forest for Classification and Regression","text":"","code":"data(iris, package = \"datasets\") tree <- ODT(Species ~ ., data = iris) #> Warning in ODT_compute(formula, Call, varName, X, y, Xsplit, split, lambda, : #> You are creating a tree for classification print(tree) #>  #> =============================================================  #> Oblique Classification Tree structure  #> ============================================================= #>  #> 1) root #>    node2)# proj1*X < 0.21 -> (leaf1 = setosa) #>    node3)  proj1*X >= 0.21 #>       node4)# proj2*X < 0.88 -> (leaf2 = versicolor) #>       node5)# proj2*X >= 0.88 -> (leaf3 = virginica) party.tree <- as.party(tree, data = iris) print(party.tree) #>  #> Model formula: #> Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width #>  #> Fitted party: #> [1] root #> |   [2] proj1*X >= 0.20707 #> |   |   [3] proj2*X >= 0.88392: virginica (n = 53, err = 5.7%) #> |   |   [4] proj2*X < 0.88392: versicolor (n = 47, err = 0.0%) #> |   [5] proj1*X < 0.20707: setosa (n = 50, err = 0.0%) #>  #> Number of inner nodes:    2 #> Number of terminal nodes: 3 forest <- ODRF(Species ~ ., data = iris, parallel = FALSE) #> Warning in ODRF_compute(formula, Call, varName, X, y, split, lambda, #> NodeRotateFun, : You are creating a forest for classification print(forest) #>  #> Call: #>  ODRF.formula(formula = Species ~ ., data = data, parallel = FALSE)  #>                Type of oblique decision random forest: classification #>                                       Number of trees: 100 #>                            OOB estimate of error rate: 4% #> Confusion matrix: #>            setosa versicolor virginica class_error #> setosa         50          0         0  0.00000000 #> versicolor      0         47         3  0.05999988 #> virginica       0          3        47  0.05999988"},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"plot-the-tree-structure-of-odt","dir":"Articles","previous_headings":"ODRF > Usage","what":"Plot the tree structure of ODT","title":"Oblique Decision Random Forest for Classification and Regression","text":"","code":"plot(tree)"},{"path":"/articles/Oblique-Decision-Random-Forest.html","id":"getting-help","dir":"Articles","previous_headings":"ODRF","what":"Getting help","title":"Oblique Decision Random Forest for Classification and Regression","text":"encounter clear bug, please file issue minimal reproducible example GitHub. Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yu Liu. Author, maintainer, copyright holder. Yingcun Xia. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Zhan H, Liu Y, Xia Y (2024). “ODRF: Consistency Oblique Decision Tree Boosting Random Forest.” arXiv preprint arXiv:2211.12653.","code":"@Article{,   title = {{ODRF}: Consistency of Oblique Decision Tree and its Boosting and Random Forest},   author = {Haoran Zhan and Yu Liu and Yingcun Xia},   journal = {arXiv preprint arXiv:2211.12653},   year = {2024}, }"},{"path":"/index.html","id":"odrf-","dir":"","previous_headings":"","what":"Oblique Decision Random Forest for Classification and Regression","title":"Oblique Decision Random Forest for Classification and Regression","text":"ODRF implements well-known Oblique Decision Tree (ODT) ODT-based Random Forest (ODRF), uses linear combinations predictors partitioning variables traditional CART Random Forest. number modifications adopted implementation; new functions also provided.","code":""},{"path":"/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Oblique Decision Random Forest for Classification and Regression","text":"ODRF R package consists following main functions: ODT() classification regression using ODT node split linear combination predictors. ODRF() classification regression implemented ODRF, ’s extension random forest based ODT() includes random forest special case. ODBT() applies feature bagging training process ODT-based boosting trees ensemble multiple boosting trees. Online() online training update existing ODT ODRF using new data sets. prune() prune ODT bottom top validation data based prediction error. print(), predict() plot() base R functions base R Package class ODT ODRF. ODRF allows users define functions find projections node, essential performance forests. also provide complete comparison analysis ODT ODRF, details available vignette(“ODRF”).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Oblique Decision Random Forest for Classification and Regression","text":"can install development version ODRF GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"liuyu-star/ODRF\")"},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Oblique Decision Random Forest for Classification and Regression","text":"show use ODRF package examples.","code":""},{"path":"/index.html","id":"classification-and-regression-using-odt-odrf-and-odbt","dir":"","previous_headings":"Usage","what":"Classification and regression using ODT, ODRF, and ODBT.","title":"Oblique Decision Random Forest for Classification and Regression","text":"Examples classification regression using ODRF ODT follows. following example, suppose training data available two batches. first batch used train ODT ODRF, second batch used update model online. error model update significantly smaller one batch data alone. Update existing ODT ODRF online. prune first judges whether error new data reduced applied, starting last leaf nodes. ODRF, argument ‘useOOB=TRUE’ uses ‘--bag’ pruning. Examples follows. Note , prune always improve efficiency number observers training set small build simple tree structure. Therefore, expand training set random numbers make prune effective.","code":"library(ODRF) #> Loading required package: partykit #> Warning: package 'partykit' was built under R version 4.2.3 #> Loading required package: grid #> Loading required package: libcoin #> Loading required package: mvtnorm data(seeds, package = \"ODRF\") set.seed(12) train <- sample(1:209, 150) seeds_train <- data.frame(seeds[train, ]) seeds_test <- data.frame(seeds[-train, ]) forest <- ODRF(varieties_of_wheat ~ ., seeds_train, split = \"gini\",    parallel = FALSE) pred <- predict(forest, seeds_test[, -8]) (e.forest <- mean(pred != seeds_test[, 8])) #> [1] 0.01694915 data(body_fat, package = \"ODRF\") train <- sample(1:252, 200) bodyfat_train <- data.frame(body_fat[train, ]) bodyfat_test <- data.frame(body_fat[-train, ]) tree <- ODT(Density ~ ., bodyfat_train, split = 'mse') pred <- predict(tree, bodyfat_test[, -1]) (e.tree <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 4.248171e-05 btree <- ODBT(Density ~ ., bodyfat_train, bodyfat_test[, -1], type = \"reg\",parallel = FALSE, model=\"ODT\",NodeRotateFun = \"RotMatPPO\") pred <- btree$results$prediction (e.btree <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 3.718075e-05 set.seed(17) index <- sample(nrow(seeds_train), floor(nrow(seeds_train) / 2)) forest1 <- ODRF(varieties_of_wheat ~ ., seeds_train[index, ],   split = \"gini\", parallel = FALSE) pred <- predict(forest1, seeds_test[, -8]) (e.forest.1 <- mean(pred != seeds_test[, 8])) #> [1] 0.03389831 forest2 <- online(forest1, seeds_train[-index, -8], seeds_train[-index, 8]) pred <- predict(forest2, seeds_test[, -8]) (e.forest.online <- mean(pred != seeds_test[, 8])) #> [1] 0.01694915 index <- seq(floor(nrow(bodyfat_train) / 2)) tree1 <- ODT(Density ~ ., bodyfat_train[index, ], split = 'mse') pred <- predict(tree1, bodyfat_test[, -1]) (e.tree.1 <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 5.057853e-05 tree2 <- online(tree1, bodyfat_train[-index, -1], bodyfat_train[-index, 1]) pred <- predict(tree2, bodyfat_test[, -1]) (e.tree.online <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 5.065922e-05 set.seed(4) bodyfat_train=rbind(as.matrix(bodyfat_train),matrix(rnorm(3000*5),5*200,15)) seeds_train=rbind(as.matrix(seeds_train),matrix(rnorm(1200*5),5*150,8)) bodyfat_train[-seq(200),1]=sample(bodyfat_train[seq(200),1],5*200,   replace = TRUE) seeds_train[-seq(150),8]=sample(seeds_train[seq(150),8],5*150,   replace = TRUE) index <- sample(nrow(seeds_train), floor(nrow(seeds_train) / 2)) forest1 <- ODRF(seeds_train[index, -8], seeds_train[index, 8],   split = \"gini\", parallel = FALSE) pred <- predict(forest1, seeds_test[, -8]) (e.forest.1 <- mean(pred != seeds_test[, 8])) #> [1] 0.1016949 forest2 <- prune(forest1, seeds_train[-index, -8], seeds_train[-index, 8],    useOOB = FALSE) pred <- predict(forest2, seeds_test[, -8]) (e.forest.prune1 <- mean(pred != seeds_test[, 8])) #> [1] 0.08474576 forest3 <- prune(forest1, seeds_train[index, -8], seeds_train[index, 8]) pred <- predict(forest3, seeds_test[, -8]) (e.forest.prune2 <- mean(pred != seeds_test[, 8])) #> [1] 0.08474576 index <- sample(nrow(bodyfat_train), floor(nrow(bodyfat_train) / 2)) tree1 <- ODT(bodyfat_train[index, -1], bodyfat_train[index, 1], split = 'mse') pred <- predict(tree1, bodyfat_test[, -1]) (e.tree.1 <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 0.0001275841 tree2 <- prune(tree1, bodyfat_train[-index, -1], bodyfat_train[-index, 1]) pred <- predict(tree2, bodyfat_test[, -1]) (e.tree.prune <- mean((pred - bodyfat_test[, 1])^2)) #> [1] 7.589647e-05"},{"path":"/index.html","id":"print-the-tree-structure-of-odt-and-odrf","dir":"","previous_headings":"Usage","what":"print the tree structure of ODT and ODRF","title":"Oblique Decision Random Forest for Classification and Regression","text":"","code":"data(iris, package = \"datasets\") tree <- ODT(Species ~ ., data = iris) #> Warning in ODT_compute(formula, Call, varName, X, y, Xsplit, split, lambda, : #> You are creating a tree for classification print(tree) #>  #> =============================================================  #> Oblique Classification Tree structure  #> ============================================================= #>  #> 1) root #>    node2)# proj1*X < 0.29 -> (leaf1 = setosa) #>    node3)  proj1*X >= 0.29 #>       node4)# proj2*X < 0.52 -> (leaf2 = versicolor) #>       node5)# proj2*X >= 0.52 -> (leaf3 = virginica) party.tree <- as.party(tree, data = iris) print(party.tree) #>  #> Model formula: #> Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width #>  #> Fitted party: #> [1] root #> |   [2] proj1*X >= 0.29165 #> |   |   [3] proj2*X >= 0.52235: virginica (n = 54, err = 7.4%) #> |   |   [4] proj2*X < 0.52235: versicolor (n = 46, err = 0.0%) #> |   [5] proj1*X < 0.29165: setosa (n = 50, err = 0.0%) #>  #> Number of inner nodes:    2 #> Number of terminal nodes: 3 forest <- ODRF(Species ~ ., data = iris, parallel = FALSE) #> Warning in ODRF_compute(formula, Call, varName, X, y, split, lambda, #> NodeRotateFun, : You are creating a forest for classification print(forest) #>  #> Call: #>  ODRF.formula(formula = Species ~ ., data = data, parallel = FALSE)  #>                Type of oblique decision random forest: classification #>                                       Number of trees: 100 #>                            OOB estimate of error rate: 4.67% #> Confusion matrix: #>            setosa versicolor virginica class_error #> setosa         50          0         0  0.00000000 #> versicolor      0         47         4  0.07843122 #> virginica       0          3        46  0.06122436"},{"path":"/index.html","id":"plot-the-tree-structure-of-odt","dir":"","previous_headings":"Usage","what":"Plot the tree structure of ODT","title":"Oblique Decision Random Forest for Classification and Regression","text":"","code":"plot(tree)"},{"path":"/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"Oblique Decision Random Forest for Classification and Regression","text":"encounter clear bug, please file issue minimal reproducible example GitHub. Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/reference/Accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"accuracy of oblique decision random forest — Accuracy","title":"accuracy of oblique decision random forest — Accuracy","text":"Prediction accuracy ODRF different tree sizes.","code":""},{"path":"/reference/Accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"accuracy of oblique decision random forest — Accuracy","text":"","code":"Accuracy(obj, data, newdata = NULL)"},{"path":"/reference/Accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"accuracy of oblique decision random forest — Accuracy","text":"obj object class ODRF, created function ODRF. data Training data class data.frame ODRF used calculate OOB error. newdata data frame matrix containing new data used calculate test error. missing, replaced data.","code":""},{"path":"/reference/Accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"accuracy of oblique decision random forest — Accuracy","text":"OOB error test error, misclassification rate (MR) classification mean square error (MSE) regression.","code":""},{"path":[]},{"path":"/reference/Accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"accuracy of oblique decision random forest — Accuracy","text":"","code":"data(breast_cancer) set.seed(221212) train <- sample(1:569, 80) train_data <- data.frame(breast_cancer[train, -1]) test_data <- data.frame(breast_cancer[-train, -1])  forest <- ODRF(diagnosis ~ ., train_data, split = \"gini\", parallel = FALSE, ntrees = 50) (error <- Accuracy(forest, train_data, test_data)) #> $err.oob #>  [1] 0.09677419 0.06382979 0.05555556 0.03389831 0.02941176 0.04166667 #>  [7] 0.05263158 0.08974359 0.07692308 0.06250000 0.06250000 0.05000000 #> [13] 0.02500000 0.05000000 0.07500000 0.06250000 0.05000000 0.06250000 #> [19] 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 #> [25] 0.06250000 0.06250000 0.06250000 0.05000000 0.05000000 0.03750000 #> [31] 0.05000000 0.05000000 0.05000000 0.05000000 0.03750000 0.05000000 #> [37] 0.05000000 0.05000000 0.03750000 0.05000000 0.03750000 0.03750000 #> [43] 0.03750000 0.03750000 0.03750000 0.03750000 0.03750000 0.03750000 #> [49] 0.03750000 0.03750000 #>  #> $err.test #>  [1] 0.12883436 0.09202454 0.05725971 0.04907975 0.06952965 0.06952965 #>  [7] 0.05112474 0.05725971 0.04498978 0.05316973 0.04907975 0.04907975 #> [13] 0.04498978 0.04498978 0.05112474 0.05521472 0.04703476 0.05112474 #> [19] 0.05316973 0.04907975 0.05316973 0.04703476 0.04907975 0.05316973 #> [25] 0.05112474 0.05521472 0.05316973 0.05521472 0.05521472 0.05725971 #> [31] 0.05725971 0.05930470 0.05521472 0.05521472 0.05521472 0.05725971 #> [37] 0.05725971 0.05316973 0.05316973 0.05521472 0.05316973 0.05316973 #> [43] 0.05316973 0.05521472 0.05521472 0.05316973 0.05316973 0.05112474 #> [49] 0.05316973 0.05112474 #>  #> $split #> [1] \"gini\" #>  #> attr(,\"class\") #> [1] \"Accuracy\""},{"path":"/reference/ODBT.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"use ODT basic tree model (base learner). improve performance boosting tree, apply feature bagging process, way random forest. final estimator called ensemble ODT-based boosting trees, denoted ODBT, average many boosting trees.","code":""},{"path":"/reference/ODBT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"","code":"ODBT(X, ...)  # S3 method for class 'formula' ODBT(   formula,   data = NULL,   Xnew = NULL,   type = \"auto\",   model = c(\"ODT\", \"rpart\", \"rpart.cpp\")[1],   TreeRotate = TRUE,   max.terms = 30,   NodeRotateFun = \"RotMatRF\",   FunDir = getwd(),   paramList = NULL,   ntrees = 100,   storeOOB = TRUE,   replacement = TRUE,   stratify = TRUE,   ratOOB = 0.368,   parallel = TRUE,   numCores = Inf,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = ceiling(sqrt(ifelse(replacement, 1, 1 - ratOOB) * ifelse(is.null(data),     length(eval(formula[[2]])), nrow(data)))/3),   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"No\",   ... )  # Default S3 method ODBT(   X,   y,   Xnew = NULL,   type = \"auto\",   model = c(\"ODT\", \"rpart\", \"rpart.cpp\")[1],   TreeRotate = TRUE,   max.terms = 30,   NodeRotateFun = \"RotMatRF\",   FunDir = getwd(),   paramList = NULL,   ntrees = 100,   storeOOB = TRUE,   replacement = TRUE,   stratify = TRUE,   ratOOB = 0.368,   parallel = TRUE,   numCores = Inf,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = ceiling(sqrt(ifelse(replacement, 1, 1 - ratOOB) * length(y))/3),   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"No\",   ... )"},{"path":"/reference/ODBT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"X n d numeric matrix (preferable) data frame. ... Optional parameters passed low level function. formula Object class formula response describing model fit. data frame, taken model frame. (see model.frame) data Training data class data.frame containing variables named formula. data missing obtained current environment formula. Xnew n d numeric matrix (preferable) data frame containing predictors new data. type Use ODBT classification (\"class\") regression (\"reg\").'auto' (default): response data y factor, \"class\" used, otherwise regression assumed. model basic tree model boosting. offer three options: \"ODT\" (default), \"rpart,\" \"rpart.cpp\" (improved \"rpart\").. TreeRotate rotate training data rotation matrix estimated logistic regression building tree (default TRUE). max.terms maximum number iterations boosting trees. NodeRotateFun Name function class character implements linear combination predictors split node. including \"RotMatPPO\": projection pursuit optimization model (PPO), see RotMatPPO (default, model=\"PPR\"). \"RotMatRF\": single feature similar Random Forest, see RotMatRF. \"RotMatRand\": random rotation, see RotMatRand. \"RotMatMake\": users can define function, details see RotMatMake. FunDir path function user-defined NodeRotateFun (default current working directory). paramList List parameters used functions NodeRotateFun. left unchanged, default values used, details see defaults. ntrees number trees forest (default 100). storeOOB TRUE samples omitted creation tree stored part tree (default TRUE). replacement TRUE n samples chosen, replacement, training data (default TRUE). stratify TRUE class sample proportions maintained random sampling. Ignored replacement = FALSE (default TRUE). ratOOB Ratio '--bag' (default 1/3). parallel Parallel computing (default TRUE). numCores Number cores used parallel computing (default Inf). MaxDepth maximum depth tree (default Inf). numNode Number nodes can used tree (default Inf). MinLeaf Minimal node size (Default 5). subset index vector indicating rows used. (NOTE: given, argument must named.) weights Vector non-negative observational weights; fractional weights allowed (default NULL). na.action function specify action taken NAs found. (NOTE: given, argument must named.) catLabel category labels class list predictors. (default NULL, details see Examples) Xcat class vector used indicate predictor categorical variable, default Xcat=0 means special treatment given category variables. Xcat=NULL, predictor x satisfies condition (length(unique(x))<10) & (n>20) judged category variable. Xscale Predictor standardization methods. \" Min-max\" (default), \"Quantile\", \"\" denote Min-max transformation, Quantile transformation transformation respectively. y response vector length n.","code":""},{"path":"/reference/ODBT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"object class ODBT Containing list components: call: original call ODBT. terms: object class c(\"terms\", \"formula\") (see terms.object) summarizing formula. Used various methods, typically direct relevance users. ppTrees: tree used build forest. oobErr: '--bag' error tree, misclassification rate (MR) classification mean square error (MSE) regression. oobIndex: training data use '--bag'. oobPred: Predicted value '--bag'. : tree related values ODT. oobErr: '--bag' error forest, misclassification rate (MR) classification mean square error (MSE) regression. oobConfusionMat: '--bag' confusion matrix forest. split, Levels NodeRotateFun important parameters building tree. paramList: Parameters named list used NodeRotateFun. data: list data related parameters used build forest. tree: list tree related parameters used build tree. forest: list forest related parameters used build forest. results: prediction results new data Xnew using ODBT.","code":""},{"path":"/reference/ODBT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"Zhan, H., Liu, Y., & Xia, Y. (2024). Consistency Oblique Decision Tree Boosting Random Forest. arXiv preprint arXiv:2211.12653. Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... & Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal machine learning research, 21(104).","code":""},{"path":[]},{"path":"/reference/ODBT.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Classification and Regression using the Ensemble of ODT-based Boosting Trees — ODBT","text":"Yu Liu Yingcun Xia","code":""},{"path":[]},{"path":"/reference/ODRF.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification and Regression using Oblique Decision Random Forest — ODRF","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"Classification regression implemented oblique decision random forest. ODRF usually produces accurate predictions RF, needs longer computation time.","code":""},{"path":"/reference/ODRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"","code":"ODRF(X, ...)  # S3 method for class 'formula' ODRF(   formula,   data = NULL,   split = \"auto\",   lambda = \"log\",   NodeRotateFun = \"RotMatPPO\",   FunDir = getwd(),   paramList = NULL,   ntrees = 100,   storeOOB = TRUE,   replacement = TRUE,   stratify = TRUE,   ratOOB = 1/3,   parallel = TRUE,   numCores = Inf,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = 5,   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"Min-max\",   TreeRandRotate = FALSE,   ... )  # Default S3 method ODRF(   X,   y,   split = \"auto\",   lambda = \"log\",   NodeRotateFun = \"RotMatPPO\",   FunDir = getwd(),   paramList = NULL,   ntrees = 100,   storeOOB = TRUE,   replacement = TRUE,   stratify = TRUE,   ratOOB = 1/3,   parallel = TRUE,   numCores = Inf,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = 5,   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"Min-max\",   TreeRandRotate = FALSE,   ... )"},{"path":"/reference/ODRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"X n d numeric matrix (preferable) data frame. ... Optional parameters passed low level function. formula Object class formula response describing model fit. data frame, taken model frame. (see model.frame) data Training data class data.frame containing variables named formula. data missing obtained current environment formula. split criterion used splitting nodes. \"entropy\": information gain \"gini\": gini impurity index classification; \"mse\": mean square error regression; 'auto' (default): response data y factor, \"gini\" used, otherwise regression assumed. lambda argument split used determine penalty level partition criterion. Three options provided including, lambda=0: penalty; lambda=2: AIC penalty; lambda='log' (Default): BIC penalty. Addition, lambda can value 0 n (training set size). NodeRotateFun Name function class character implements linear combination predictors split node. including \"RotMatPPO\": projection pursuit optimization model (PPO), see RotMatPPO (default, model=\"PPR\"). \"RotMatRF\": single feature similar Random Forest, see RotMatRF. \"RotMatRand\": random rotation, see RotMatRand. \"RotMatMake\": users can define function, details see RotMatMake. FunDir path function user-defined NodeRotateFun (default current working directory). paramList List parameters used functions NodeRotateFun. left unchanged, default values used, details see defaults. ntrees number trees forest (default 100). storeOOB TRUE samples omitted creation tree stored part tree (default TRUE). replacement TRUE n samples chosen, replacement, training data (default TRUE). stratify TRUE class sample proportions maintained random sampling. Ignored replacement = FALSE (default TRUE). ratOOB Ratio '--bag' (default 1/3). parallel Parallel computing (default TRUE). numCores Number cores used parallel computing (default Inf). MaxDepth maximum depth tree (default Inf). numNode Number nodes can used tree (default Inf). MinLeaf Minimal node size (Default 5). subset index vector indicating rows used. (NOTE: given, argument must named.) weights Vector non-negative observational weights; fractional weights allowed (default NULL). na.action function specify action taken NAs found. (NOTE: given, argument must named.) catLabel category labels class list predictors. (default NULL, details see Examples) Xcat class vector used indicate predictor categorical variable. default Xcat=0 means special treatment given category variables. Xcat=NULL, predictor x satisfies condition \"(length(table(x))<10) & (length(x)>20)\" judged category variable. Xscale Predictor standardization methods. \" Min-max\" (default), \"Quantile\", \"\" denote Min-max transformation, Quantile transformation transformation respectively. TreeRandRotate randomly rotate training data building tree (default FALSE, see RandRot). y response vector length n.","code":""},{"path":"/reference/ODRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"object class ODRF Containing list components: call: original call ODRF. terms: object class c(\"terms\", \"formula\") (see terms.object) summarizing formula. Used various methods, typically direct relevance users. split, Levels NodeRotateFun important parameters building tree. predicted: predicted values training data based --bag samples. paramList: Parameters named list used NodeRotateFun. oobErr: '--bag' error forest, misclassification rate (MR) classification mean square error (MSE) regression. oobConfusionMat: '--bag' confusion matrix forest. structure: tree structure used build forest. oobErr: '--bag' error tree, misclassification rate (MR) classification mean square error (MSE) regression. oobIndex: training data use '--bag'. oobPred: Predicted value '--bag'. others: tree structure return value ODT. data: list data related parameters used build forest. tree: list tree related parameters used build tree. forest: list forest related parameters used build forest.","code":""},{"path":"/reference/ODRF.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"Zhan, H., Liu, Y., & Xia, Y. (2022). Consistency Oblique Decision Tree Random Forest. arXiv preprint arXiv:2211.12653. Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... & Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal machine learning research, 21(104).","code":""},{"path":[]},{"path":"/reference/ODRF.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"Yu Liu Yingcun Xia","code":""},{"path":"/reference/ODRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classification and Regression using Oblique Decision Random Forest — ODRF","text":"","code":"# Classification with Oblique Decision Randome Forest. data(seeds) set.seed(221212) train <- sample(1:209, 80) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) forest <- ODRF(varieties_of_wheat ~ ., train_data,   split = \"entropy\",parallel = FALSE, ntrees = 50 ) pred <- predict(forest, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.03875969 # \\donttest{ # Regression with Oblique Decision Randome Forest. data(body_fat) set.seed(221212) train <- sample(1:252, 80) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) forest <- ODRF(Density ~ ., train_data,   split = \"mse\", parallel = FALSE,   NodeRotateFun = \"RotMatPPO\", paramList = list(model = \"Log\", dimProj = \"Rand\") ) pred <- predict(forest, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 5.840115e-05 # }  ### Train ODRF on one-of-K encoded categorical data ### # Note that the category variable must be placed at the beginning of the predictor X # as in the following example. set.seed(22) Xcol1 <- sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE) Xcol2 <- sample(c(\"1\", \"2\", \"3\", \"4\", \"5\"), 100, replace = TRUE) Xcon <- matrix(rnorm(100 * 3), 100, 3) X <- data.frame(Xcol1, Xcol2, Xcon) Xcat <- c(1, 2) catLabel <- NULL y <- as.factor(sample(c(0, 1), 100, replace = TRUE)) # \\donttest{ forest <- ODRF(y ~ X, split = \"entropy\", Xcat = NULL, parallel = FALSE) #> Error in eval(formula[[3]]): object 'X' not found # } head(X) #>   Xcol1 Xcol2          X1         X2          X3 #> 1     B     5 -0.04178453  2.3962339 -0.01443979 #> 2     A     4 -1.66084623 -0.4397486  0.57251733 #> 3     B     2 -0.57973333 -0.2878683  1.24475578 #> 4     B     1 -0.82075051  1.3702900  0.01716528 #> 5     C     5 -0.76337897 -0.9620213  0.25846351 #> 6     A     5 -0.37720294 -0.1853976  1.04872159 #>   Xcol1 Xcol2          X1         X2          X3 #> 1     B     5 -0.04178453  2.3962339 -0.01443979 #> 2     A     4 -1.66084623 -0.4397486  0.57251733 #> 3     B     2 -0.57973333 -0.2878683  1.24475578 #> 4     B     1 -0.82075051  1.3702900  0.01716528 #> 5     C     5 -0.76337897 -0.9620213  0.25846351 #> 6     A     5 -0.37720294 -0.1853976  1.04872159  # one-of-K encode each categorical feature and store in X1 numCat <- apply(X[, Xcat, drop = FALSE], 2, function(x) length(unique(x))) # initialize training data matrix X1 X1 <- matrix(0, nrow = nrow(X), ncol = sum(numCat)) catLabel <- vector(\"list\", length(Xcat)) names(catLabel) <- colnames(X)[Xcat] col.idx <- 0L # convert categorical feature to K dummy variables for (j in seq_along(Xcat)) {   catMap <- (col.idx + 1):(col.idx + numCat[j])   catLabel[[j]] <- levels(as.factor(X[, Xcat[j]]))   X1[, catMap] <- (matrix(X[, Xcat[j]], nrow(X), numCat[j]) ==     matrix(catLabel[[j]], nrow(X), numCat[j], byrow = TRUE)) + 0   col.idx <- col.idx + numCat[j] } X <- cbind(X1, X[, -Xcat]) colnames(X) <- c(paste(rep(seq_along(numCat), numCat), unlist(catLabel),   sep = \".\" ), \"X1\", \"X2\", \"X3\")  # Print the result after processing of category variables. head(X) #>   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3 #> 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979 #> 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733 #> 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578 #> 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528 #> 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351 #> 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159 #>   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3 #> 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979 #> 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733 #> 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578 #> 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528 #> 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351 #> 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159 catLabel #> $Xcol1 #> [1] \"A\" \"B\" \"C\" #>  #> $Xcol2 #> [1] \"1\" \"2\" \"3\" \"4\" \"5\" #>  #> $Xcol1 #> [1] \"A\" \"B\" \"C\" #> #> $Xcol2 #> [1] \"1\" \"2\" \"3\" \"4\" \"5\"  # \\donttest{ forest <- ODRF(X, y,   split = \"gini\", Xcat = c(1, 2),   catLabel = catLabel, parallel = FALSE ) # }"},{"path":"/reference/ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification and Regression with Oblique Decision Tree — ODT","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"Classification regression using oblique decision tree (ODT) node split linear combination predictors. Different methods provided selecting linear combinations, splitting values chosen one three criteria.","code":""},{"path":"/reference/ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"","code":"ODT(X, ...)  # S3 method for class 'formula' ODT(   formula,   data = NULL,   Xsplit = NULL,   split = \"auto\",   lambda = \"log\",   NodeRotateFun = \"RotMatPPO\",   FunDir = getwd(),   paramList = NULL,   glmnetParList = NULL,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = 10,   Levels = NULL,   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"Min-max\",   TreeRandRotate = FALSE,   ... )  # Default S3 method ODT(   X,   y,   Xsplit = NULL,   split = \"auto\",   lambda = \"log\",   NodeRotateFun = \"RotMatPPO\",   FunDir = getwd(),   paramList = NULL,   glmnetParList = NULL,   MaxDepth = Inf,   numNode = Inf,   MinLeaf = 10,   Levels = NULL,   subset = NULL,   weights = NULL,   na.action = na.fail,   catLabel = NULL,   Xcat = 0,   Xscale = \"Min-max\",   TreeRandRotate = FALSE,   ... )"},{"path":"/reference/ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"X n d numeric matrix (preferable) data frame. ... Optional parameters passed low level function. formula Object class formula response describing model fit. data frame, taken model frame. (see model.frame) data Training data class data.frame containing variables named formula. data missing obtained current environment formula. Xsplit Splitting variables used construct linear model trees. default value NULL valid split=\"linear\". split criterion used splitting nodes. \"entropy\": information gain \"gini\": gini impurity index classification; \"mse\": mean square error regression; \"linear\": mean square error linear model. 'auto' (default): response data y factor, \"gini\" used, otherwise \"mse\" assumed. lambda argument split used determine penalty level partition criterion. Three options provided including, lambda=0: penalty; lambda=2: AIC penalty; lambda='log' (Default): BIC penalty. Addition, lambda can value 0 n (training set size). NodeRotateFun Name function class character implements linear combination predictors split node. including \"RotMatPPO\": projection pursuit optimization model (PPO), see RotMatPPO (default, model=\"PPR\"). \"RotMatRF\": single feature similar CART, see RotMatRF. \"RotMatRand\": random rotation, see RotMatRand. \"RotMatMake\": users can define function, details see RotMatMake. FunDir path function user-defined NodeRotateFun (default current working directory). paramList List parameters used functions NodeRotateFun. left unchanged, default values used, details see defaults. glmnetParList List parameters used functions glmnet cv.glmnet package glmnet. glmnetParList=list(lambda = 0) Ordinary Least Squares (OLS) regression, glmnetParList=list(family = \"gaussian\") (default) regression model glmnetParList=list(family = \"binomial\" \"multinomial\") classification model. left unchanged, default values used, details see glmnet cv.glmnet. MaxDepth maximum depth tree (default Inf). numNode Number nodes can used tree (default Inf). MinLeaf Minimal node size (Default 10). Levels category label response variable split equal 'mse'. subset index vector indicating rows used. (NOTE: given, argument must named.) weights Vector non-negative observational weights; fractional weights allowed (default NULL). na.action function specify action taken NAs found. (NOTE: given, argument must named.) catLabel category labels class list predictors. (default NULL, details see Examples) Xcat class vector used indicate predictor categorical variable. default Xcat=0 means special treatment given category variables. Xcat=NULL, predictor x satisfies condition \"(length(table(x))<10) & (length(x)>20)\" judged category variable. Xscale Predictor standardization methods. \" Min-max\" (default), \"Quantile\", \"\" denote Min-max transformation, Quantile transformation transformation respectively. TreeRandRotate randomly rotate training data building tree (default FALSE, see RandRot). y response vector length n.","code":""},{"path":"/reference/ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"object class ODT containing list components:: call: original call ODT. terms: object class c(\"terms\", \"formula\") (see terms.object) summarizing formula. Used various methods, typically direct relevance users. split, Levels NodeRotateFun important parameters building tree. predicted: predicted values training data. projections: Projection direction split node. paramList: Parameters named list used NodeRotateFun. data: list data related parameters used build tree. tree: list tree related parameters used build tree. structure: set tree structure data records. nodeRotaMat: Record split variables (first column), split node serial number (second column) rotation direction (third column) node. (first column third column 0 means leaf nodes) nodeNumLabel: Record leaf node's category classification predicted value regression (second column data size). (column 0 means leaf node) nodeCutValue: Record split point node. (0 means leaf nodes) nodeCutIndex: Record index values partitioning variables selected based partition criterion split. childNode: Record number child nodes splitting. nodeDepth: Record depth tree node located. nodeIndex: Record indices data used node. glmnetFit: Record model fitted function glmnet used node.","code":""},{"path":"/reference/ODT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"Zhan, H., Liu, Y., & Xia, Y. (2022). Consistency Oblique Decision Tree Random Forest. arXiv preprint arXiv:2211.12653.","code":""},{"path":[]},{"path":"/reference/ODT.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"Yu Liu Yingcun Xia","code":""},{"path":"/reference/ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classification and Regression with Oblique Decision Tree — ODT","text":"","code":"# Classification with Oblique Decision Tree. data(seeds) set.seed(221212) train <- sample(1:209, 100) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) tree <- ODT(varieties_of_wheat ~ ., train_data, split = \"entropy\") pred <- predict(tree, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.09174312  # Regression with Oblique Decision Tree. data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) tree <- ODT(Density ~ ., train_data,   split = \"mse\",   NodeRotateFun = \"RotMatPPO\", paramList = list(model = \"Log\", dimProj = \"Rand\") ) pred <- predict(tree, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 0.0005496953  # Use \"Z\" as the splitting variable to build a linear model tree for \"X\" and \"y\". set.seed(10) cutpoint=50 X=matrix(rnorm(100*10),100,10) age=sample(seq(20,80),100,replace = TRUE) height=sample(seq(50,200),100,replace = TRUE) weight=sample(seq(5,150),100,replace = TRUE) Z=cbind(age=age,height=height,weight=weight) mu=rep(0,100) mu[age<=cutpoint]=X[age<=cutpoint,1]+X[age<=cutpoint,2] mu[age>cutpoint]=X[age>cutpoint,1]+X[age>cutpoint,3] y=mu+rnorm(100) # Regression model tree my.tree <- ODT(X=X, y=y, Xsplit=Z, split = \"linear\", lambda = 0, NodeRotateFun = \"RotMatRF\", glmnetParList=list(lambda = 0, family = \"gaussian\")) pred <- predict(my.tree, X, Xsplit=Z) # fitting error mean((pred - y)^2) #> [1] 0.9035932 mean((my.tree$predicted - y)^2) #> [1] 0.9035932 # Classification model tree y1 = (y>0)*1 my.tree <- ODT(X=X, y=y1, Xsplit=Z, split = \"linear\",lambda = 0,                NodeRotateFun = \"RotMatRF\",MinLeaf = 10, MaxDepth = 5,                glmnetParList=list(family = \"binomial\")) #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground (class <- predict(my.tree, X, Xsplit=Z, type=\"pred\")) #> Warning: number of rows of result is not a multiple of vector length (arg 1) #>   [1] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" #>  [19] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" #>  [37] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" #>  [55] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" #>  [73] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" #>  [91] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" (prob <- predict(my.tree, X, Xsplit=Z, type=\"prob\")) #> Warning: number of rows of result is not a multiple of vector length (arg 1) #>        0 1 #>   [1,] 1 0 #>   [2,] 1 0 #>   [3,] 1 0 #>   [4,] 1 0 #>   [5,] 1 0 #>   [6,] 1 0 #>   [7,] 1 0 #>   [8,] 1 0 #>   [9,] 1 0 #>  [10,] 1 0 #>  [11,] 1 0 #>  [12,] 1 0 #>  [13,] 1 0 #>  [14,] 1 0 #>  [15,] 1 0 #>  [16,] 1 0 #>  [17,] 1 0 #>  [18,] 1 0 #>  [19,] 1 0 #>  [20,] 1 0 #>  [21,] 1 0 #>  [22,] 1 0 #>  [23,] 1 0 #>  [24,] 1 0 #>  [25,] 1 0 #>  [26,] 1 0 #>  [27,] 1 0 #>  [28,] 1 0 #>  [29,] 1 0 #>  [30,] 1 0 #>  [31,] 1 0 #>  [32,] 1 0 #>  [33,] 1 0 #>  [34,] 1 0 #>  [35,] 1 0 #>  [36,] 1 0 #>  [37,] 1 0 #>  [38,] 1 0 #>  [39,] 1 0 #>  [40,] 1 0 #>  [41,] 1 0 #>  [42,] 1 0 #>  [43,] 1 0 #>  [44,] 1 0 #>  [45,] 1 0 #>  [46,] 1 0 #>  [47,] 1 0 #>  [48,] 1 0 #>  [49,] 1 0 #>  [50,] 1 0 #>  [51,] 1 0 #>  [52,] 1 0 #>  [53,] 1 0 #>  [54,] 1 0 #>  [55,] 1 0 #>  [56,] 1 0 #>  [57,] 1 0 #>  [58,] 1 0 #>  [59,] 1 0 #>  [60,] 1 0 #>  [61,] 1 0 #>  [62,] 1 0 #>  [63,] 1 0 #>  [64,] 1 0 #>  [65,] 1 0 #>  [66,] 1 0 #>  [67,] 1 0 #>  [68,] 1 0 #>  [69,] 1 0 #>  [70,] 1 0 #>  [71,] 1 0 #>  [72,] 1 0 #>  [73,] 1 0 #>  [74,] 1 0 #>  [75,] 1 0 #>  [76,] 1 0 #>  [77,] 1 0 #>  [78,] 1 0 #>  [79,] 1 0 #>  [80,] 1 0 #>  [81,] 1 0 #>  [82,] 1 0 #>  [83,] 1 0 #>  [84,] 1 0 #>  [85,] 1 0 #>  [86,] 1 0 #>  [87,] 1 0 #>  [88,] 1 0 #>  [89,] 1 0 #>  [90,] 1 0 #>  [91,] 1 0 #>  [92,] 1 0 #>  [93,] 1 0 #>  [94,] 1 0 #>  [95,] 1 0 #>  [96,] 1 0 #>  [97,] 1 0 #>  [98,] 1 0 #>  [99,] 1 0 #> [100,] 1 0  # Projection analysis of the oblique decision tree. data(iris) tree <- ODT(Species ~ ., data = iris, split=\"gini\",             paramList = list(model = \"PPR\", numProj = 1)) print(round(tree[[\"projections\"]],3)) #>       Sepal.Length Sepal.Width Petal.Length Petal.Width #> proj1       -0.167      -0.215        0.763       0.587 #> proj2       -0.087      -0.223        0.676       0.697  ### Train ODT on one-of-K encoded categorical data ### # Note that the category variable must be placed at the beginning of the predictor X # as in the following example. set.seed(22) Xcol1 <- sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE) Xcol2 <- sample(c(\"1\", \"2\", \"3\", \"4\", \"5\"), 100, replace = TRUE) Xcon <- matrix(rnorm(100 * 3), 100, 3) X <- data.frame(Xcol1, Xcol2, Xcon) Xcat <- c(1, 2) catLabel <- NULL y <- as.factor(sample(c(0, 1), 100, replace = TRUE)) tree <- ODT(X, y, split = \"entropy\", Xcat = NULL) #> Warning: The categorical variable 1, 2 has been transformed into an one-of-K encode variables! head(X) #>   Xcol1 Xcol2          X1         X2          X3 #> 1     B     5 -0.04178453  2.3962339 -0.01443979 #> 2     A     4 -1.66084623 -0.4397486  0.57251733 #> 3     B     2 -0.57973333 -0.2878683  1.24475578 #> 4     B     1 -0.82075051  1.3702900  0.01716528 #> 5     C     5 -0.76337897 -0.9620213  0.25846351 #> 6     A     5 -0.37720294 -0.1853976  1.04872159 #>   Xcol1 Xcol2          X1         X2          X3 #> 1     B     5 -0.04178453  2.3962339 -0.01443979 #> 2     A     4 -1.66084623 -0.4397486  0.57251733 #> 3     B     2 -0.57973333 -0.2878683  1.24475578 #> 4     B     1 -0.82075051  1.3702900  0.01716528 #> 5     C     5 -0.76337897 -0.9620213  0.25846351 #> 6     A     5 -0.37720294 -0.1853976  1.04872159  # one-of-K encode each categorical feature and store in X1 numCat <- apply(X[, Xcat, drop = FALSE], 2, function(x) length(unique(x))) # initialize training data matrix X X1 <- matrix(0, nrow = nrow(X), ncol = sum(numCat)) catLabel <- vector(\"list\", length(Xcat)) names(catLabel) <- colnames(X)[Xcat] col.idx <- 0L # convert categorical feature to K dummy variables for (j in seq_along(Xcat)) {   catMap <- (col.idx + 1):(col.idx + numCat[j])   catLabel[[j]] <- levels(as.factor(X[, Xcat[j]]))   X1[, catMap] <- (matrix(X[, Xcat[j]], nrow(X), numCat[j]) ==     matrix(catLabel[[j]], nrow(X), numCat[j], byrow = TRUE)) + 0   col.idx <- col.idx + numCat[j] } X <- cbind(X1, X[, -Xcat]) colnames(X) <- c(paste(rep(seq_along(numCat), numCat), unlist(catLabel),   sep = \".\" ), \"X1\", \"X2\", \"X3\")  # Print the result after processing of category variables. head(X) #>   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3 #> 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979 #> 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733 #> 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578 #> 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528 #> 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351 #> 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159 #>   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3 #> 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979 #> 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733 #> 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578 #> 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528 #> 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351 #> 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159 catLabel #> $Xcol1 #> [1] \"A\" \"B\" \"C\" #>  #> $Xcol2 #> [1] \"1\" \"2\" \"3\" \"4\" \"5\" #>  #> $Xcol1 #> [1] \"A\" \"B\" \"C\" #> #> $Xcol2 #> [1] \"1\" \"2\" \"3\" \"4\" \"5\"  tree <- ODT(X, y, split = \"gini\", Xcat = c(1, 2), catLabel = catLabel,NodeRotateFun = \"RotMatRF\")"},{"path":"/reference/PPO.html","id":null,"dir":"Reference","previous_headings":"","what":"Projection Pursuit Optimization — PPO","title":"Projection Pursuit Optimization — PPO","text":"Find optimal projection using various projectin pursuit models.","code":""},{"path":"/reference/PPO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Projection Pursuit Optimization — PPO","text":"","code":"PPO(X, y, model = \"PPR\", split = \"gini\", weights = NULL, ...)"},{"path":"/reference/PPO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Projection Pursuit Optimization — PPO","text":"X n d numeric matrix (preferable) data frame. y response vector length n. model Model projection pursuit. \"PPR\"(default): projection projection regression ppr. y category label, expanded K binary features. \"Log\": logistic based nnet. \"Rand\": random projection generated \\(\\{-1, 1\\}\\). following models can used classification, .e. split must ”entropy” 'gini'. \"LDA\", \"PDA\", \"Lr\", \"GINI\", \"ENTROPY\" library PPtreeViz. following models based Pursuit. \"holes\": Holes index \"cm\": Central Mass index \"holes\": Holes index \"friedmantukey\": Friedman Tukey index \"legendre\": Legendre index \"laguerrefourier\": Laguerre Fourier index \"hermite\": Hermite index \"naturalhermite\": Natural Hermite index \"kurtosismax\": Maximum kurtosis index \"kurtosismin\": Minimum kurtosis index \"moment\": Moment index \"mf\": MF index \"chi\": Chi-square index split criterion used splitting variable. 'gini': gini impurity index (classification, default), 'entropy': information gain (classification) 'mse': mean square error (regression). weights Vector non-negative observational weights; fractional weights allowed (default NULL). ... optional parameters passed low level function.","code":""},{"path":"/reference/PPO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Projection Pursuit Optimization — PPO","text":"Optimal projection direction.","code":""},{"path":"/reference/PPO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Projection Pursuit Optimization — PPO","text":"Friedman, J. H., & Stuetzle, W. (1981). Projection pursuit regression. Journal American statistical Association, 76(376), 817-823. Ripley, B. D. (1996) Pattern Recognition Neural Networks. Cambridge. Lee, YD, Cook, D., Park JW, Lee, EK(2013) PPtree: Projection Pursuit Classification Tree, Electronic Journal Statistics, 7:1369-1386. Cook, D., Buja, ., Lee, E. K., & Wickham, H. (2008). Grand tours, projection pursuit guided tours, manual controls. Handbook data visualization (pp. 295-314). Springer, Berlin, Heidelberg.","code":""},{"path":[]},{"path":"/reference/PPO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Projection Pursuit Optimization — PPO","text":"","code":"# classification data(seeds) (PP <- PPO(seeds[, 1:7], seeds[, 8], model = \"Log\", split = \"entropy\")) #> [1]  -2.309383   2.847194  31.625381  35.243198   3.104818  -1.524764 -36.500103 (PP <- PPO(seeds[, 1:7], seeds[, 8], model = \"PPR\", split = \"entropy\")) #> [1] -0.04663995 -0.01700724 -0.92771564 -0.23227466  0.20425324  0.01478790 #> [7]  0.20245882 (PP <- PPO(seeds[, 1:7], seeds[, 8], model = \"LDA\", split = \"entropy\")) #> [1] -0.18579584 -0.38830262  0.79096768 -0.23851549 -0.33785977  0.02884637 #> [7] -0.13114931  # regression data(body_fat) (PP <- PPO(body_fat[, 2:15], body_fat[, 1], model = \"Log\", split = \"mse\")) #>  [1]  0.576428167 -0.660665448 -0.064453715  0.525631193 -0.472349313 #>  [6]  0.536074208 -0.123738306  0.473947709 -0.011705711  0.382371005 #> [11]  0.003063609 -0.412904055  0.583918320  0.593973852 (PP <- PPO(body_fat[, 2:15], body_fat[, 1], model = \"Rand\", split = \"mse\")) #>  [1] -1  1 -1 -1  1  1  1  1  1  1  1  1 -1 -1 (PP <- PPO(body_fat[, 2:15], body_fat[, 1], model = \"PPR\", split = \"mse\")) #>  [1] -0.973615195  0.007627492  0.018938292 -0.002055055  0.013834544 #>  [6]  0.030809259 -0.069084891  0.039733024 -0.039890401 -0.005993149 #> [11] -0.107760827 -0.075608376 -0.006239274  0.158635551"},{"path":"/reference/RandRot.html","id":null,"dir":"Reference","previous_headings":"","what":"Samples a p x p uniformly random rotation matrix — RandRot","title":"Samples a p x p uniformly random rotation matrix — RandRot","text":"Samples p x p uniformly random rotation matrix via QR decomposition matrix elements sampled iid standard normal distribution.","code":""},{"path":"/reference/RandRot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Samples a p x p uniformly random rotation matrix — RandRot","text":"","code":"RandRot(p)"},{"path":"/reference/RandRot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Samples a p x p uniformly random rotation matrix — RandRot","text":"p columns n p numeric matrix data frame.","code":""},{"path":"/reference/RandRot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Samples a p x p uniformly random rotation matrix — RandRot","text":"p x p uniformly random rotation matrix.","code":""},{"path":[]},{"path":"/reference/RandRot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Samples a p x p uniformly random rotation matrix — RandRot","text":"","code":"set.seed(220828) (RandRot(10)) #>               [,1]         [,2]        [,3]        [,4]        [,5]       [,6] #>  [1,]  0.004089438  0.092784118 -0.04768148  0.29468196  0.27523552  0.5569504 #>  [2,]  0.663646964  0.009810906  0.55264934 -0.06325634  0.24661496  0.0294133 #>  [3,] -0.020873539 -0.063554137 -0.40145339  0.50696035 -0.05420152  0.1860007 #>  [4,]  0.066758845  0.178588115 -0.29990998 -0.68088711  0.11939604  0.2496002 #>  [5,]  0.388689304 -0.055940018 -0.33624068  0.13538789  0.09979185 -0.2133235 #>  [6,] -0.082892327 -0.535972020 -0.18785759 -0.03513325  0.71666347 -0.3026466 #>  [7,]  0.272220201  0.331995688 -0.12946882  0.17075487  0.32464169  0.3110954 #>  [8,]  0.095942954 -0.475896114 -0.19936269 -0.33182589 -0.14450619  0.4848229 #>  [9,]  0.328928834 -0.537895427  0.09331814  0.16202069 -0.38116729  0.1238993 #> [10,]  0.453001055  0.197324556 -0.47710602 -0.05886047 -0.22606533 -0.3282654 #>             [,7]        [,8]        [,9]        [,10] #>  [1,]  0.2406591 -0.12293853  0.64072530 -0.180984786 #>  [2,] -0.2248341  0.34223682  0.10951828  0.093268660 #>  [3,] -0.3824591  0.61037413 -0.14087146 -0.033781333 #>  [4,] -0.1023218  0.21420220 -0.07151225 -0.521607407 #>  [5,] -0.5360606 -0.57437687  0.10210402 -0.176758014 #>  [6,]  0.2148600  0.13219435 -0.02147308 -0.005936893 #>  [7,]  0.2761220 -0.21561482 -0.64521037  0.168977955 #>  [8,] -0.1304103 -0.10355794  0.03794325  0.573897093 #>  [9,]  0.3376664 -0.05444565 -0.21272763 -0.494610777 #> [10,]  0.4404470  0.20434887  0.28043285  0.226936364"},{"path":"/reference/RotMatMake.html","id":null,"dir":"Reference","previous_headings":"","what":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"Create projection matrix self-defined projection matrix function projection optimization model function","code":""},{"path":"/reference/RotMatMake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"","code":"RotMatMake(   X = NULL,   y = NULL,   RotMatFun = \"RotMatPPO\",   PPFun = \"PPO\",   FunDir = getwd(),   paramList = NULL,   ... )"},{"path":"/reference/RotMatMake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"X n d numeric matrix (preferable) data frame. y response vector length n. RotMatFun self-defined projection matrix function name, can also RotMatRand RotMatPPO. Note (,...) necessary. PPFun self-defined projection function name, can also PPO. Note (,...) necessary. FunDir path function user-defined NodeRotateFun (default current Workspace). paramList List parameters used functions RotMatFun PPFun. left unchanged, default values used, details see defaults. ... Used handle superfluous arguments passed using paramList.","code":""},{"path":"/reference/RotMatMake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"random matrix use running ODT. Variable: Variables projected. Number: Number projections. Coefficient: Coefficients projection matrix.","code":""},{"path":"/reference/RotMatMake.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"two ways user define projection direction function. first way connect two custom functions function RotMatMake(). Specifically, RotMatFun() defined determine variables projected, projection dimensions number projections (first two columns rotation matrix). PPFun() defined determine projection coefficients (third column rotation matrix). let argument RotMatFun=\"RotMatMake\", argument paramList must contain parameters RotMatFun PPFun. second way define function directly, just let argument RotMatFun name defined function let argument paramList arguments list used defined function.","code":""},{"path":[]},{"path":"/reference/RotMatMake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create rotation matrix used to determine the linear combination of features. — RotMatMake","text":"","code":"set.seed(220828) X <- matrix(rnorm(1000), 100, 10) y <- (rnorm(100) > 0) + 0 (RotMat <- RotMatMake(X, y, \"RotMatRand\", \"PPO\")) #>       Variable Number Coefficient #>  [1,]        7      1  1.00000000 #>  [2,]        1      2  0.57384117 #>  [3,]        3      2  0.11546398 #>  [4,]        4      2  0.38222074 #>  [5,]        6      2  0.71503964 #>  [6,]        5      3  0.26515379 #>  [7,]        6      3  0.65980321 #>  [8,]       10      3 -0.70310255 #>  [9,]        5      4  0.27464916 #> [10,]        7      4 -0.02355140 #> [11,]        8      4 -0.96087481 #> [12,]        9      4  0.02706962 library(nnet) (RotMat <- RotMatMake(X, y, \"RotMatPPO\", \"PPO\", paramList = list(model = \"Log\"))) #>       Variable Number Coefficient #>  [1,]       10      1   1.0000000 #>  [2,]        5      2   1.0000000 #>  [3,]        3      3   1.0000000 #>  [4,]        2      4   1.0000000 #>  [5,]        4      5   3.4905900 #>  [6,]        1      5  -6.6609352 #>  [7,]       10      5   4.2280817 #>  [8,]        7      5  -6.4552493 #>  [9,]        6      5   4.5486872 #> [10,]        8      6 -16.5737922 #> [11,]        3      6 -23.3417386 #> [12,]        5      6  12.5920425 #> [13,]        2      6  12.6463337 #> [14,]        9      6  60.5034813 #> [15,]        8      7  -2.5041262 #> [16,]        9      7  -0.1602014 #> [17,]       10      7   1.1502737 #> [18,]        6      7   1.2121139 #> [19,]        1      7   0.2975510  ## Define projection matrix function makeRotMat and projection pursuit function makePP.## ##  Note that '...' is necessary. makeRotMat <- function(dimX, dimProj, numProj, ...) {   RotMat <- matrix(1, dimProj * numProj, 3)   for (np in seq(numProj)) {     RotMat[(dimProj * (np - 1) + 1):(dimProj * np), 1] <-       sample(1:dimX, dimProj, replace = FALSE)     RotMat[(dimProj * (np - 1) + 1):(dimProj * np), 2] <- np   }   return(RotMat) }  makePP <- function(dimProj, prob, ...) {   pp <- sample(c(1L, -1L), dimProj, replace = TRUE, prob = c(prob, 1 - prob))   return(pp) } # \\donttest{ RotMat <- RotMatMake(   RotMatFun = \"makeRotMat\", PPFun = \"makePP\",   paramList = list(dimX = 8, dimProj = 5, numProj = 4, prob = 0.5) ) #> Warning: cannot open file '/home/runner/work/ODRF/ODRF/docs/reference/makeRotMat.R': No such file or directory #> Error in file(filename, \"r\", encoding = encoding): cannot open the connection head(RotMat) #>      Variable Number Coefficient #> [1,]       10      1    1.000000 #> [2,]        5      2    1.000000 #> [3,]        3      3    1.000000 #> [4,]        2      4    1.000000 #> [5,]        4      5    3.490590 #> [6,]        1      5   -6.660935 #>      Variable Number Coefficient #> [1,]        6      1           1 #> [2,]        8      1           1 #> [3,]        1      1          -1 #> [4,]        4      1          -1 #> [5,]        5      1          -1 #> [6,]        6      2           1 # } # \\donttest{ # train ODT with defined projection matrix function tree <- ODT(X, y,   split = \"entropy\", NodeRotateFun = \"makeRotMat\",   paramList = list(dimX = ncol(X), dimProj = 5, numProj = 4) ) #> Warning: cannot open file '/home/runner/work/ODRF/ODRF/docs/reference/makeRotMat.R': No such file or directory #> Error in file(filename, \"r\", encoding = encoding): cannot open the connection # train ODT with defined projection matrix function and projection optimization model function tree <- ODT(X, y,   split = \"entropy\", NodeRotateFun = \"RotMatMake\", paramList =     list(       RotMatFun = \"makeRotMat\", PPFun = \"makePP\",       dimX = ncol(X), dimProj = 5, numProj = 4, prob = 0.5     ) ) #> Warning: cannot open file '/home/runner/work/ODRF/ODRF/docs/reference/makeRotMat.R': No such file or directory #> Error in file(filename, \"r\", encoding = encoding): cannot open the connection # }"},{"path":"/reference/RotMatPPO.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Projection Matrix: RotMatPPO — RotMatPPO","title":"Create a Projection Matrix: RotMatPPO — RotMatPPO","text":"Create projection matrix using projection pursuit optimization (PPO).","code":""},{"path":"/reference/RotMatPPO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Projection Matrix: RotMatPPO — RotMatPPO","text":"","code":"RotMatPPO(   X,   y,   model = \"PPR\",   split = \"entropy\",   weights = NULL,   dimProj = min(ceiling(length(y)^0.4), ceiling(ncol(X) * 2/3)),   numProj = ifelse(dimProj == \"Rand\", sample(floor(ncol(X)/3), 1),     ceiling(ncol(X)/dimProj)),   catLabel = NULL,   ... )"},{"path":"/reference/RotMatPPO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Projection Matrix: RotMatPPO — RotMatPPO","text":"X n d numeric matrix (preferable) data frame. y response vector length n. model Model projection pursuit (details see PPO). split One three criteria, 'gini': gini impurity index (classification), 'entropy': information gain (classification, default) 'mse': mean square error (regression). weights vector length data positive weights. (default NULL) dimProj Number variables projected, dimProj=min(ceiling(n^0.4),ceiling(ncol(X)*2/3)) (default) dimProj=\"Rand\": random 1 ncol(X). numProj number projection directions, dimProj=\"Rand\" default numProj = sample(ceiling(ncol(X)/3),1) otherwise default numProj=ceiling(ncol(X)/dimProj). catLabel category labels class list predictors. (default NULL, details see Examples ODT) ... Used handle superfluous arguments passed using paramList.","code":""},{"path":"/reference/RotMatPPO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Projection Matrix: RotMatPPO — RotMatPPO","text":"random matrix use running ODT. Variable: Variables projected. Number: Number projections. Coefficient: Coefficients projection matrix.","code":""},{"path":[]},{"path":"/reference/RotMatPPO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Projection Matrix: RotMatPPO — RotMatPPO","text":"","code":"set.seed(220828) X <- matrix(rnorm(1000), 100, 10) y <- (rnorm(100) > 0) + 0 (RotMat <- RotMatPPO(X, y)) #>       Variable Number Coefficient #>  [1,]        7      1  1.00000000 #>  [2,]        3      2  1.00000000 #>  [3,]        2      3  1.00000000 #>  [4,]       10      4  1.00000000 #>  [5,]        8      5 -0.63747080 #>  [6,]        7      5 -0.33498954 #>  [7,]        6      5  0.40102900 #>  [8,]        4      5  0.44271368 #>  [9,]       10      5 -0.35297779 #> [10,]        2      6 -0.22464801 #> [11,]        1      6  0.70400011 #> [12,]        5      6  0.16604464 #> [13,]        3      6 -0.04357413 #> [14,]        9      6  0.65149643 #> [15,]        1      7  0.62001403 #> [16,]        9      7  0.26056728 #> [17,]        8      7 -0.12300744 #> [18,]        4      7  0.40539513 #> [19,]        6      7  0.60680413 (RotMat <- RotMatPPO(X, y, dimProj = \"Rand\")) #>       Variable Number Coefficient #>  [1,]        9      1  1.00000000 #>  [2,]        3      2  1.00000000 #>  [3,]        8      3  1.00000000 #>  [4,]        1      4  1.00000000 #>  [5,]        1      1  0.41034620 #>  [6,]        2      1 -0.05702688 #>  [7,]        3      1 -0.20189258 #>  [8,]        4      1  0.24738638 #>  [9,]        5      1  0.18941654 #> [10,]        6      1  0.33597211 #> [11,]        7      1 -0.19232749 #> [12,]        8      1 -0.11345253 #> [13,]        9      1  0.36297960 #> [14,]       10      1 -0.62931064 (RotMat <- RotMatPPO(X, y, dimProj = 6, numProj = 4)) #>       Variable Number Coefficient #>  [1,]        8      1   1.0000000 #>  [2,]        1      2   1.0000000 #>  [3,]        2      3   1.0000000 #>  [4,]        6      4   1.0000000 #>  [5,]        5      5   0.7524546 #>  [6,]        2      5  -0.2513190 #>  [7,]        4      5   0.6088110 #>  [8,]        1      6   0.8235489 #>  [9,]        8      6  -0.5672452 #> [10,]        6      7   0.8527041 #> [11,]        3      7  -0.5223943 #> [12,]       10      8  -0.8067687 #> [13,]        9      8   0.4836435 #> [14,]        7      8  -0.3394308 #> [15,]        6      9   0.5429000 #> [16,]        1      9   0.3816742 #> [17,]       10      9  -0.6710470 #> [18,]        5      9   0.3305759  # classification data(seeds) (PP <- RotMatPPO(seeds[, 1:7], seeds[, 8], model = \"Log\", split = \"entropy\")) #>       Variable Number Coefficient #>  [1,]        7      1  1.00000000 #>  [2,]        6      2  1.00000000 #>  [3,]        5      3  1.00000000 #>  [4,]        7      4  0.80070860 #>  [5,]        2      4 -0.57197028 #>  [6,]        6      4  0.09180371 #>  [7,]        5      4 -0.15260345 #>  [8,]        3      5 -0.99266473 #>  [9,]        1      5 -0.01373195 #> [10,]        4      5 -0.12011731 #> [11,]        3      6 -0.92259750 #> [12,]        7      6  0.22777345 #> [13,]        2      6 -0.23433424 #> [14,]        5      6  0.20498921 (PP <- RotMatPPO(seeds[, 1:7], seeds[, 8], model = \"PPR\", split = \"entropy\")) #>       Variable Number  Coefficient #>  [1,]        3      1  1.000000000 #>  [2,]        1      2  1.000000000 #>  [3,]        2      3  1.000000000 #>  [4,]        3      4 -0.988281570 #>  [5,]        6      4  0.008187774 #>  [6,]        1      4  0.048662943 #>  [7,]        2      4 -0.144445205 #>  [8,]        5      5 -0.291498857 #>  [9,]        7      5  0.548547148 #> [10,]        4      5 -0.783660924 #> [11,]        3      6 -0.848820653 #> [12,]        4      6 -0.442538941 #> [13,]        7      6  0.282088117 #> [14,]        5      6 -0.063945909 (PP <- RotMatPPO(seeds[, 1:7], seeds[, 8], model = \"LDA\", split = \"entropy\")) #>       Variable Number Coefficient #>  [1,]        6      1  1.00000000 #>  [2,]        7      2  1.00000000 #>  [3,]        5      3  1.00000000 #>  [4,]        5      4  0.84899943 #>  [5,]        1      4  0.42679608 #>  [6,]        6      4 -0.05823805 #>  [7,]        7      4  0.30602845 #>  [8,]        2      5 -0.42409016 #>  [9,]        3      5 -0.90512230 #> [10,]        4      5 -0.03001925 #> [11,]        3      6  0.92258733 #> [12,]        5      6  0.17607151 #> [13,]        1      6 -0.04931458 #> [14,]        2      6  0.33970504  # regression data(body_fat) (PP <- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = \"Log\", split = \"mse\")) #>       Variable Number   Coefficient #>  [1,]       12      1  1.0000000000 #>  [2,]        8      2  1.0000000000 #>  [3,]        9      3  1.0000000000 #>  [4,]       14      4  1.0000000000 #>  [5,]        5      5 -0.1354890116 #>  [6,]       10      5 -0.0229807153 #>  [7,]       14      5  0.9318006142 #>  [8,]       11      5 -0.0487637387 #>  [9,]       12      5 -0.1148845383 #> [10,]        2      5 -0.0924499623 #> [11,]        9      5 -0.2978907099 #> [12,]        7      6 -0.0425456052 #> [13,]        4      6  0.0137041007 #> [14,]        6      6  0.0316612459 #> [15,]        8      6 -0.0108830294 #> [16,]        3      6  0.0107094563 #> [17,]        1      6 -0.9976644991 #> [18,]       13      6 -0.0378424231 #> [19,]        1      7 -0.9668121838 #> [20,]       14      7  0.2391075465 #> [21,]        9      7 -0.0053082782 #> [22,]        5      7  0.0443749634 #> [23,]       12      7 -0.0441831784 #> [24,]        2      7  0.0006183072 #> [25,]       11      7 -0.0644354848 (PP <- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = \"Rand\", split = \"mse\")) #>       Variable Number Coefficient #>  [1,]       12      1   1.0000000 #>  [2,]        5      2   1.0000000 #>  [3,]        3      3   1.0000000 #>  [4,]        4      4   1.0000000 #>  [5,]        5      5   0.3779645 #>  [6,]        9      5  -0.3779645 #>  [7,]       12      5  -0.3779645 #>  [8,]       14      5   0.3779645 #>  [9,]        1      5   0.3779645 #> [10,]       10      5   0.3779645 #> [11,]        8      5  -0.3779645 #> [12,]        7      6  -0.3779645 #> [13,]        4      6  -0.3779645 #> [14,]        2      6  -0.3779645 #> [15,]       13      6   0.3779645 #> [16,]       11      6  -0.3779645 #> [17,]        3      6   0.3779645 #> [18,]        6      6   0.3779645 #> [19,]        5      7   0.3779645 #> [20,]        9      7  -0.3779645 #> [21,]       12      7   0.3779645 #> [22,]       14      7  -0.3779645 #> [23,]        1      7  -0.3779645 #> [24,]       10      7   0.3779645 #> [25,]        8      7   0.3779645 (PP <- RotMatPPO(body_fat[, 2:15], body_fat[, 1], model = \"PPR\", split = \"mse\")) #>       Variable Number  Coefficient #>  [1,]        8      1  1.000000000 #>  [2,]       10      2  1.000000000 #>  [3,]        5      3  1.000000000 #>  [4,]        6      4  1.000000000 #>  [5,]        7      5 -0.356875669 #>  [6,]        6      5  0.037456061 #>  [7,]       13      5 -0.186967615 #>  [8,]        2      5 -0.027215019 #>  [9,]       14      5  0.883994707 #> [10,]        5      5  0.215317575 #> [11,]       10      5  0.087925847 #> [12,]       12      6 -0.045902870 #> [13,]        3      6  0.018505068 #> [14,]        1      6 -0.994007545 #> [15,]        4      6 -0.000279017 #> [16,]        9      6 -0.057642171 #> [17,]       11      6 -0.077971676 #> [18,]        8      6  0.009859475 #> [19,]        1      7 -0.966846911 #> [20,]       14      7  0.235875959 #> [21,]        7      7 -0.010618395 #> [22,]        5      7  0.029799022 #> [23,]       13      7 -0.024434531 #> [24,]       10      7  0.026277615 #> [25,]       11      7 -0.085330484"},{"path":"/reference/RotMatRF.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","title":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","text":"Create projection matrix coefficient 1 0 ODRF (ODT) partition variables Random Forest (CART).","code":""},{"path":"/reference/RotMatRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","text":"","code":"RotMatRF(dimX, numProj, catLabel = NULL, ...)"},{"path":"/reference/RotMatRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","text":"dimX number dimensions. numProj number projection directions (default ceiling(sqrt(dimX))). catLabel category labels class list predictors. (default NULL, details see Examples ODT) ... Used handle superfluous arguments passed using paramList.","code":""},{"path":"/reference/RotMatRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","text":"random matrix use running ODT. Variable: Variables projected. Number: Number projections. Coefficient: Coefficients projection matrix.","code":""},{"path":[]},{"path":"/reference/RotMatRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Projection Matrix: Random Forest (RF) — RotMatRF","text":"","code":"paramList <- list(dimX = 8, numProj = 3, catLabel = NULL) set.seed(2) (RotMat <- do.call(RotMatRF, paramList)) #>      Variable Number Coefficient #> [1,]        5      1           1 #> [2,]        7      2           1 #> [3,]        6      3           1"},{"path":"/reference/RotMatRand.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Rotation Matrix — RotMatRand","title":"Random Rotation Matrix — RotMatRand","text":"Generate rotation matrices different distributions, comes library rerf.","code":""},{"path":"/reference/RotMatRand.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Rotation Matrix — RotMatRand","text":"","code":"RotMatRand(   dimX,   randDist = \"Binary\",   numProj = ceiling(sqrt(dimX)),   dimProj = \"Rand\",   sparsity = ifelse(dimX >= 10, 3/dimX, 1/dimX),   prob = 0.5,   lambda = 1,   catLabel = NULL,   ... )"},{"path":"/reference/RotMatRand.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Rotation Matrix — RotMatRand","text":"dimX number dimensions. randDist probability distribution random projection direction, including \"Binary\": \\(B\\{-1,1\\}\\) binomial distribution (default), \"Norm\":\\(N(0,1)\\) normal distribution, \"Uniform\": \\(U(-1,1)\\) uniform distribution. numProj number projection directions (default ceiling(sqrt(dimX))). dimProj Number variables projected, default dimProj=\"Rand\": random 1 dimX. sparsity real number \\((0,1)\\) specifies distribution non-zero elements random matrix. sparsity=\"pois\" means non-zero elements generated p(lambda) Poisson distribution. prob probability \\((0,1)\\) used sampling \\({-1,1}\\) prob = 0 sample -1 prob = 1 sample 1. lambda Parameter Poisson distribution (default 1). catLabel category labels class list predictors. (default NULL, details see Examples ODT) ... Used handle superfluous arguments passed using paramList.","code":""},{"path":"/reference/RotMatRand.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random Rotation Matrix — RotMatRand","text":"random matrix use running ODT. Variable: Variables projected. Number: Number projections. Coefficient: Coefficients projection matrix.","code":""},{"path":"/reference/RotMatRand.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random Rotation Matrix — RotMatRand","text":"Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... & Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal machine learning research, 21(104).","code":""},{"path":[]},{"path":"/reference/RotMatRand.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random Rotation Matrix — RotMatRand","text":"","code":"set.seed(1) paramList <- list(dimX = 8, numProj = 3, sparsity = 0.25, prob = 0.5) (RotMat <- do.call(RotMatRand, paramList)) #>      Variable Number Coefficient #> [1,]        1      1           1 #> [2,]        2      1           1 #> [3,]        4      1           1 #> [4,]        7      1          -1 #> [5,]        3      2           1 #> [6,]        6      2          -1 paramList <- list(dimX = 8, numProj = 3, sparsity = \"pois\") (RotMat <- do.call(RotMatRand, paramList)) #>      Variable Number Coefficient #> [1,]        2      1           1 #> [2,]        7      2           1 #> [3,]        1      3           1 #> [4,]        7      3           1 #> [5,]        5      3          -1 #> [6,]        6      3           1 paramList <- list(dimX = 8, randDist = \"Norm\", dimProj = 5) (RotMat <- do.call(RotMatRand, paramList)) #>       Variable Number Coefficient #>  [1,]        2      1  0.80418951 #>  [2,]        5      1 -0.05710677 #>  [3,]        1      1  0.50360797 #>  [4,]        4      1  1.08576936 #>  [5,]        3      1 -0.69095384 #>  [6,]        1      2 -1.28459935 #>  [7,]        4      2  0.04672617 #>  [8,]        3      2 -0.23570656 #>  [9,]        2      2 -0.54288826 #> [10,]        5      2 -0.43331032 #> [11,]        2      3 -0.64947165 #> [12,]        5      3  0.72675075 #> [13,]        3      3  1.15191175 #> [14,]        4      3  0.99216037 #> [15,]        1      3 -0.42951311"},{"path":"/reference/VarImp.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract variable importance measure — VarImp","title":"Extract variable importance measure — VarImp","text":"extractor function variable importance measures produced ODT ODRF.","code":""},{"path":"/reference/VarImp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract variable importance measure — VarImp","text":"","code":"VarImp(obj, X = NULL, y = NULL, type = \"permutation\")"},{"path":"/reference/VarImp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract variable importance measure — VarImp","text":"obj object class ODT ODRF. X n d numerical matrix (preferably) data frame used ODRF. y response vector length n used ODRF. type specifying type importance measure. \"impurity\": mean decrease node impurity, \"permutation\" (default): mean decrease accuracy.","code":""},{"path":"/reference/VarImp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract variable importance measure — VarImp","text":"matrix importance measure, first column predictors second column Increased error. Misclassification rate (MR) classification mean square error (MSE) regression. larger increased error important variable .","code":""},{"path":"/reference/VarImp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract variable importance measure — VarImp","text":"note randomForest package, definitions variable importance measures. first measure total decrease node impurities splitting variable, averaged trees. classification, node impurity measured Gini index. regression, measured residual sum squares. second measure computed permuting OOB data: tree, prediction error --bag portion data recorded. done permuting predictor variable. difference two averaged trees.","code":""},{"path":[]},{"path":"/reference/VarImp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract variable importance measure — VarImp","text":"","code":"data(body_fat) y=body_fat[,1] X=body_fat[,-1]  tree <- ODT(X, y, split = \"mse\") (varimp <- VarImp(tree, type=\"impurity\")) #> $varImp #>         varible decrease_accuracy #> BodyFat       1      5.985098e-03 #> Hip           8      1.388494e-05 #> Abdomen       7      8.268897e-06 #> Wrist        14      7.487498e-06 #> Height        4      7.191170e-06 #> Ankle        11      4.633147e-06 #> Biceps       12      3.746768e-06 #> Knee         10      1.684495e-06 #> Age           2      1.568993e-07 #> Neck          5      1.045500e-07 #> Thigh         9      8.405915e-08 #> Forearm      13      6.621724e-08 #> Chest         6      1.472028e-08 #> Weight        3      1.198327e-09 #>  #> $split #> [1] \"mse\" #>  #> attr(,\"class\") #> [1] \"VarImp\"  forest <- ODRF(X, y, split = \"mse\", parallel = FALSE, ntrees=50) (varimp <- VarImp(forest, type=\"impurity\")) #> $varImp #>         varible decrease_accuracy #> BodyFat       1      3.536018e-03 #> Weight        3      3.454507e-05 #> Abdomen       7      1.430011e-05 #> Height        4      1.294990e-05 #> Thigh         9      9.089121e-06 #> Ankle        11      6.935579e-06 #> Hip           8      6.807814e-06 #> Chest         6      5.835472e-06 #> Neck          5      4.937802e-06 #> Wrist        14      3.528603e-06 #> Knee         10      1.820711e-06 #> Biceps       12      1.444451e-06 #> Forearm      13      8.782072e-07 #> Age           2      3.991877e-07 #>  #> $split #> [1] \"mse\" #>  #> attr(,\"class\") #> [1] \"VarImp\" (varimp <- VarImp(forest, X, y, type=\"permutation\")) #> $varImp #>         varible decrease_accuracy #> BodyFat       1      6.895483e-04 #> Weight        3      3.540741e-06 #> Thigh         9      1.286703e-06 #> Hip           8      9.485006e-07 #> Ankle        11      7.919036e-07 #> Knee         10      6.297860e-07 #> Abdomen       7      5.934018e-07 #> Neck          5      5.512524e-07 #> Chest         6      5.354998e-07 #> Age           2      4.518536e-07 #> Forearm      13      1.970960e-07 #> Height        4     -2.007132e-07 #> Wrist        14     -3.065903e-07 #> Biceps       12     -4.805390e-07 #>  #> $split #> [1] \"mse\" #>  #> attr(,\"class\") #> [1] \"VarImp\""},{"path":"/reference/as.party.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"ODT as party — as.party.ODT","title":"ODT as party — as.party.ODT","text":"make ODT object objects class party.","code":""},{"path":"/reference/as.party.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ODT as party — as.party.ODT","text":"","code":"# S3 method for class 'ODT' as.party(obj, data, ...)"},{"path":"/reference/as.party.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ODT as party — as.party.ODT","text":"obj object class ODT. data Training data class data.frame used convert object class ODT, must training data data ODT. ... Arguments passed methods","code":""},{"path":"/reference/as.party.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ODT as party — as.party.ODT","text":"objects class party.","code":""},{"path":"/reference/as.party.ODT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ODT as party — as.party.ODT","text":"Lee, EK(2017) PPtreeViz: R Package Visualizing Projection Pursuit Classification Trees, Journal Statistical Software.","code":""},{"path":[]},{"path":"/reference/as.party.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ODT as party — as.party.ODT","text":"","code":"data(iris) tree <- ODT(Species ~ ., data = iris) #> Warning: You are creating a tree for classification tree #>  #> =============================================================  #> Oblique Classification Tree structure  #> ============================================================= #>  #> 1) root #>    node2)# proj1*X < 0.25 -> (leaf1 = setosa) #>    node3)  proj1*X >= 0.25 #>       node4)# proj2*X < 0.52 -> (leaf2 = versicolor) #>       node5)# proj2*X >= 0.52 -> (leaf3 = virginica) plot(tree)  party.tree <- as.party(tree, data = iris) party.tree #>  #> Model formula: #> Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width #>  #> Fitted party: #> [1] root #> |   [2] proj1*X >= 0.24576 #> |   |   [3] proj2*X >= 0.52235: virginica (n = 54, err = 7.4%) #> |   |   [4] proj2*X < 0.52235: versicolor (n = 46, err = 0.0%) #> |   [5] proj1*X < 0.24576: setosa (n = 50, err = 0.0%) #>  #> Number of inner nodes:    2 #> Number of terminal nodes: 3 plot(party.tree)"},{"path":"/reference/best.cut.node.html","id":null,"dir":"Reference","previous_headings":"","what":"find best splitting variable and node — best.cut.node","title":"find best splitting variable and node — best.cut.node","text":"function select splitting variables nodes using one four criteria.","code":""},{"path":"/reference/best.cut.node.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"find best splitting variable and node — best.cut.node","text":"","code":"best.cut.node(   X,   y,   Xsplit = X,   split,   lambda = \"log\",   weights = 1,   MinLeaf = 10,   numLabels = ifelse(split %in% c(\"gini\", \"entropy\"), length(unique(y)), 0),   glmnetParList = NULL )"},{"path":"/reference/best.cut.node.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"find best splitting variable and node — best.cut.node","text":"X n d numeric matrix (preferable) data frame. y response vector length n. Xsplit Splitting variables used construct linear model trees. default value NULL valid split=\"linear\". split criterion used splitting nodes. \"entropy\": information gain \"gini\": gini impurity index classification; \"\": mean square error regression; \"linear\": mean square error multiple linear regression. lambda argument split used determine penalty level partition criterion. Three options provided including, lambda=0: penalty; lambda=2: AIC penalty; lambda='log' (Default): BIC penalty. Addition, lambda can value 0 n (training set size). weights vector values weigh samples considering split. MinLeaf Minimal node size (Default 10). numLabels number categories. glmnetParList List parameters used functions glmnet cv.glmnet package glmnet. left unchanged, default values used, details see glmnet cv.glmnet.","code":""},{"path":"/reference/best.cut.node.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"find best splitting variable and node — best.cut.node","text":"list contains: BestCutVar: best split variable. BestCutVal: best split points best split variable. BestIndex: variable corresponds maximum decrease gini impurity index, information gain, mean square error. fitL fitR: multivariate linear models left right nodes splitting trained using function glmnet.","code":""},{"path":"/reference/best.cut.node.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"find best splitting variable and node — best.cut.node","text":"","code":"### Find the best split variable ### #Classification data(iris) X <- as.matrix(iris[, 1:4]) y <- iris[[5]] (bestcut <- best.cut.node(X, y, split = \"gini\")) #> $BestCutVar #> [1] 3 #>  #> $BestCutVal #> [1] 2.45 #>  #> $BestIndex #> [1] 33.70157 17.41287 52.08715 52.08715 #>  (bestcut <- best.cut.node(X, y, split = \"entropy\")) #> $BestCutVar #> [1] 3 #>  #> $BestCutVal #> [1] 2.45 #>  #> $BestIndex #> [1] 38169.08 38169.08 38169.08 38169.08 #>   #Regression data(body_fat) X=body_fat[, -1] y=body_fat[, 1] (bestcut <- best.cut.node(X, y, split = \"mse\")) #> $BestCutVar #> [1] 1 #>  #> $BestCutVal #> [1] 18.65 #>  #> $BestIndex #>  [1]  0.063141385  0.004454660  0.024274816 -0.002024766  0.016210553 #>  [6]  0.027524967  0.042242211  0.021316394  0.017042437  0.013917053 #> [11]  0.005538530  0.018069179  0.011068821  0.005087794 #>   set.seed(10) cutpoint=50 X=matrix(rnorm(100*10),100,10) age=sample(seq(20,80),100,replace = TRUE) height=sample(seq(50,200),100,replace = TRUE) weight=sample(seq(5,150),100,replace = TRUE) Xsplit=cbind(age=age,height=height,weight=weight) mu=rep(0,100) mu[age<=cutpoint]=X[age<=cutpoint,1]+X[age<=cutpoint,2] mu[age>cutpoint]=X[age>cutpoint,1]+X[age>cutpoint,3] y=mu+rnorm(100) bestcut <- best.cut.node(X, y, Xsplit, split = \"linear\",            glmnetParList=list(lambda = 0))"},{"path":"/reference/body_fat.html","id":null,"dir":"Reference","previous_headings":"","what":"Body Fat Prediction Dataset — body_fat","title":"Body Fat Prediction Dataset — body_fat","text":"Lists estimates percentage body fat determined underwater weighing various body circumference measurements 252 men. Accurate measurement body fat inconvenient/costly desirable easy methods estimating body fat inconvenient/costly.","code":""},{"path":"/reference/body_fat.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Body Fat Prediction Dataset — body_fat","text":"data frame 252 rows 15 covariate variables 1 response variable","code":""},{"path":"/reference/body_fat.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Body Fat Prediction Dataset — body_fat","text":"https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset","code":""},{"path":"/reference/body_fat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Body Fat Prediction Dataset — body_fat","text":"variables listed , left right, : Density determined underwater weighing Age (years) Weight (lbs) Height (inches) Neck circumference (cm) Chest circumference (cm) Abdomen 2 circumference (cm) Hip circumference (cm) Thigh circumference (cm) Knee circumference (cm) Ankle circumference (cm) Biceps (extended) circumference (cm) Forearm circumference (cm) Wrist circumference (cm)","code":""},{"path":"/reference/body_fat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Body Fat Prediction Dataset — body_fat","text":"Bailey, Covert (1994). Smart Exercise: Burning Fat, Getting Fit, Houghton-Mifflin Co., Boston, pp. 179-186.","code":""},{"path":[]},{"path":"/reference/body_fat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Body Fat Prediction Dataset — body_fat","text":"","code":"data(body_fat) set.seed(221212) train <- sample(1:252, 60) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) # \\donttest{ forest <- ODRF(Density ~ ., train_data, split = \"mse\", parallel = FALSE, ntrees = 50) pred <- predict(forest, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 3.522026e-05 # } tree <- ODT(Density ~ ., train_data, split = \"mse\") pred <- predict(tree, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 5.776313e-05"},{"path":"/reference/breast_cancer.html","id":null,"dir":"Reference","previous_headings":"","what":"Breast Cancer Dataset — breast_cancer","title":"Breast Cancer Dataset — breast_cancer","text":"Breast cancer common cancer amongst women world. accounts \\(25\\%\\) cancer cases, affected 2.1 Million people 2015 alone. starts cells breast begin grow control. cells usually form tumors can seen via X-ray felt lumps breast area. key challenges detection classify tumors malignant (cancerous) benign(non cancerous).","code":""},{"path":"/reference/breast_cancer.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Breast Cancer Dataset — breast_cancer","text":"data frame 569 rows 30 covariate variables 1 response variable","code":""},{"path":"/reference/breast_cancer.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Breast Cancer Dataset — breast_cancer","text":"https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset?select=breast-cancer.csv https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)","code":""},{"path":"/reference/breast_cancer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Breast Cancer Dataset — breast_cancer","text":"actual linear program used obtain separating plane 3-dimensional space described : ID number Diagnosis (M = malignant, B = benign) Ten real-valued features computed cell nucleus: radius (mean distances center points perimeter) texture (standard deviation gray-scale values) perimeter area smoothness (local variation radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity concave portions contour) concave points (number concave portions contour) symmetry fractal dimension (\"coastline approximation\" - 1)","code":""},{"path":"/reference/breast_cancer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Breast Cancer Dataset — breast_cancer","text":"Wolberg WH, Street WN, Mangasarian OL. Machine learning techniques diagnose breast cancer image-processed nuclear features fine needle aspirates. Cancer Lett. 1994 Mar 15;77(2-3):163-71.","code":""},{"path":[]},{"path":"/reference/breast_cancer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Breast Cancer Dataset — breast_cancer","text":"","code":"data(breast_cancer) set.seed(221212) train <- sample(1:569, 80) train_data <- data.frame(breast_cancer[train, -1]) test_data <- data.frame(breast_cancer[-train, -1])  forest <- ODRF(diagnosis ~ ., train_data, split = \"gini\", parallel = FALSE, ntrees = 50) pred <- predict(forest, test_data[, -1]) # classification error (mean(pred != test_data[, 1])) #> [1] 0.05316973  tree <- ODT(diagnosis ~ ., train_data, split = \"gini\") pred <- predict(tree, test_data[, -1]) # classification error (mean(pred != test_data[, 1])) #> [1] 0.06952965"},{"path":"/reference/defaults.html","id":null,"dir":"Reference","previous_headings":"","what":"Default values passed to RotMat* — defaults","title":"Default values passed to RotMat* — defaults","text":"Given parameter list categorical map function populates values parameter list accoding 'best' known general use case parameters.","code":""},{"path":"/reference/defaults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default values passed to RotMat* — defaults","text":"","code":"defaults(   paramList,   split = \"entropy\",   dimX = NULL,   weights = NULL,   catLabel = NULL )"},{"path":"/reference/defaults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default values passed to RotMat* — defaults","text":"paramList list (possibly empty), populated set default values passed RotMat* function. split criterion used splitting variable. 'gini': gini impurity index (classification, default), 'entropy': information gain (classification) 'mse': mean square error (regression). dimX integer denoting number columns design matrix X. weights vector length data positive weights.(default NULL) catLabel category labels class list predictors. (default NULL, details see Examples ODT)","code":""},{"path":"/reference/defaults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Default values passed to RotMat* — defaults","text":"Default parameters RotMat* function. dimX integer denoting number columns design matrix X. dimProj Number variables projected, default dimProj=\"Rand\": random 1 ncol(X). numProj number projection directions.(default ceiling(sqrt(dimX))) catLabel category labels class list prediction variables, details see Examples ODRF. weights vector length data positive weights.(default NULL) lambda Parameter Poisson distribution (default 1). sparsity real number \\((0,1)\\) specifies distribution non-zero elements random matrix. sparsity=\"pois\" means non-zero elements generated p(lambda) Poisson distribution. prob probability \\(\\(0,1)\\) used sampling . randDist Parameter Poisson distribution (default 1). split criterion used splitting variable. 'gini': gini impurity index (classification, default), 'entropy': information gain (classification) 'mse': mean square error (regression). model Model projection pursuit. (see PPO)","code":""},{"path":[]},{"path":"/reference/defaults.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Default values passed to RotMat* — defaults","text":"","code":"set.seed(1) paramList <- list(dimX = 8, numProj = 3, sparsity = 0.25, prob = 0.5) (paramList <- defaults(paramList, split = \"entropy\")) #> $dimX #> [1] 8 #>  #> $numProj #> [1] 3 #>  #> $sparsity #> [1] 0.25 #>  #> $prob #> [1] 0.5 #>  #> $dimProj #> [1] \"Rand\" #>  #> $lambda #> [1] 1 #>  #> $randDist #> [1] \"Binary\" #>  #> $split #> [1] \"entropy\" #>  #> $model #> [1] \"PPR\" #>"},{"path":"/reference/online.ODRF.html","id":null,"dir":"Reference","previous_headings":"","what":"using new training data to update an existing ODRF. — online.ODRF","title":"using new training data to update an existing ODRF. — online.ODRF","text":"Update existing ODRF using new data improve model.","code":""},{"path":"/reference/online.ODRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"using new training data to update an existing ODRF. — online.ODRF","text":"","code":"# S3 method for class 'ODRF' online(obj, X, y, weights = NULL, MaxDepth = Inf, ...)"},{"path":"/reference/online.ODRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"using new training data to update an existing ODRF. — online.ODRF","text":"obj object class ODRF. X new n d numeric matrix (preferable) data frame  used update object class ODRF. y new response vector length n used update object class ODRF. weights vector non-negative observational weights; fractional weights allowed (default NULL). MaxDepth maximum depth tree (default Inf). ... Optional parameters passed low level function.","code":""},{"path":"/reference/online.ODRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"using new training data to update an existing ODRF. — online.ODRF","text":"result ODRF.","code":""},{"path":[]},{"path":"/reference/online.ODRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"using new training data to update an existing ODRF. — online.ODRF","text":"","code":"# Classification with Oblique Decision Random Forest data(seeds) set.seed(221212) train <- sample(1:209, 80) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) index <- seq(floor(nrow(train_data) / 2)) forest <- ODRF(varieties_of_wheat ~ ., train_data[index, ],   split = \"gini\", parallel = FALSE, ntrees = 50 ) online_forest <- online(forest, train_data[-index, -8], train_data[-index, 8]) pred <- predict(online_forest, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.03875969  # Regression with Oblique Decision Random Forest # \\donttest{ data(body_fat) set.seed(221212) train <- sample(1:252, 80) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) index <- seq(floor(nrow(train_data) / 2)) forest <- ODRF(Density ~ ., train_data[index, ],   split = \"mse\", parallel = FALSE ) online_forest <- online(   forest, train_data[-index, -1],   train_data[-index, 1] ) pred <- predict(online_forest, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 4.503374e-05 # }"},{"path":"/reference/online.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"using new training data to update an existing ODT. — online.ODT","title":"using new training data to update an existing ODT. — online.ODT","text":"Update existing ODT using new data improve model.","code":""},{"path":"/reference/online.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"using new training data to update an existing ODT. — online.ODT","text":"","code":"# S3 method for class 'ODT' online(obj, X = NULL, y = NULL, weights = NULL, MaxDepth = Inf, ...)"},{"path":"/reference/online.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"using new training data to update an existing ODT. — online.ODT","text":"obj object class ODT. X new n d numeric matrix (preferable) data frame  used update object class ODT. y new response vector length n used update object class ODT. weights Vector non-negative observational weights; fractional weights allowed (default NULL). MaxDepth maximum depth tree (default Inf). ... optional parameters passed low level function.","code":""},{"path":"/reference/online.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"using new training data to update an existing ODT. — online.ODT","text":"result ODT.","code":""},{"path":[]},{"path":"/reference/online.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"using new training data to update an existing ODT. — online.ODT","text":"","code":"# Classification with Oblique Decision Tree data(seeds) set.seed(221212) train <- sample(1:209, 100) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) index <- seq(floor(nrow(train_data) / 2)) tree <- ODT(varieties_of_wheat ~ ., train_data[index, ], split = \"gini\") online_tree <- online(tree, train_data[-index, -8], train_data[-index, 8]) pred <- predict(online_tree, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.1009174  # Regression with Oblique Decision Tree data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) index <- seq(floor(nrow(train_data) / 2)) tree <- ODT(Density ~ ., train_data[index, ], split = \"mse\") online_tree <- online(tree, train_data[-index, -1], train_data[-index, 1]) pred <- predict(online_tree, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 6.186264e-05"},{"path":"/reference/online.html","id":null,"dir":"Reference","previous_headings":"","what":"online structure learning for class ODT and ODRF. — online","title":"online structure learning for class ODT and ODRF. — online","text":"ODT ODRF constantly updated multiple batches data optimize model. online S3 method class ODT ODRF.","code":""},{"path":"/reference/online.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"online structure learning for class ODT and ODRF. — online","text":"","code":"online(obj, ...)"},{"path":"/reference/online.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"online structure learning for class ODT and ODRF. — online","text":"obj object class ODT ODRF. ... parameters related class obj, see ODT ODRF.","code":""},{"path":"/reference/online.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"online structure learning for class ODT and ODRF. — online","text":"object class ODT ODRF.","code":""},{"path":[]},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"/reference/plot.Accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"plot method for Accuracy objects — plot.Accuracy","title":"plot method for Accuracy objects — plot.Accuracy","text":"Draw error graph class ODRF different tree sizes.","code":""},{"path":"/reference/plot.Accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot method for Accuracy objects — plot.Accuracy","text":"","code":"# S3 method for class 'Accuracy' plot(x, lty = 1, digits = NULL, main = NULL, ...)"},{"path":"/reference/plot.Accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot method for Accuracy objects — plot.Accuracy","text":"x Object class Accuracy. lty vector line types, see par. digits Integer indicating number decimal places (round) significant digits (signif) used. main main title plot. ... Arguments passed methods.","code":""},{"path":"/reference/plot.Accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot method for Accuracy objects — plot.Accuracy","text":"OOB error test error, misclassification rate (MR) classification mean square error (MSE) regression.","code":""},{"path":[]},{"path":"/reference/plot.Accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"plot method for Accuracy objects — plot.Accuracy","text":"","code":"data(breast_cancer) set.seed(221212) train <- sample(1:569, 80) train_data <- data.frame(breast_cancer[train, -1]) test_data <- data.frame(breast_cancer[-train, -1])  forest <- ODRF(diagnosis ~ ., train_data, split = \"gini\", parallel = FALSE, ntrees = 30) (error <- Accuracy(forest, train_data, test_data)) #> $err.oob #>  [1] 0.09677419 0.04255319 0.03703704 0.03389831 0.02941176 0.05555556 #>  [7] 0.05263158 0.07692308 0.06410256 0.05000000 0.01250000 0.02500000 #> [13] 0.00000000 0.01250000 0.06250000 0.03750000 0.06250000 0.03750000 #> [19] 0.05000000 0.02500000 0.05000000 0.03750000 0.03750000 0.05000000 #> [25] 0.05000000 0.03750000 0.03750000 0.03750000 0.02500000 0.03750000 #>  #> $err.test #>  [1] 0.15132924 0.12883436 0.07566462 0.07566462 0.07566462 0.05930470 #>  [7] 0.06134969 0.05316973 0.05521472 0.05930470 0.06134969 0.05521472 #> [13] 0.05725971 0.04907975 0.06339468 0.06339468 0.05930470 0.06134969 #> [19] 0.05930470 0.05521472 0.05521472 0.05316973 0.05316973 0.05725971 #> [25] 0.05725971 0.05930470 0.05725971 0.05725971 0.05521472 0.04907975 #>  #> $split #> [1] \"gini\" #>  #> attr(,\"class\") #> [1] \"Accuracy\" plot(error)"},{"path":"/reference/plot.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"to plot an oblique decision tree — plot.ODT","title":"to plot an oblique decision tree — plot.ODT","text":"Draw oblique decision tree tree structure. modified function PPtreeViz library.","code":""},{"path":"/reference/plot.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"to plot an oblique decision tree — plot.ODT","text":"","code":"# S3 method for class 'ODT' plot(x, font.size = 17, width.size = 1, xadj = 0, main = NULL, sub = NULL, ...)"},{"path":"/reference/plot.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"to plot an oblique decision tree — plot.ODT","text":"x object class ODT. font.size Font size plot width.size Size eclipse node. xadj size left right movement. main main title sub sub title ... Arguments passed methods.","code":""},{"path":"/reference/plot.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"to plot an oblique decision tree — plot.ODT","text":"Tree Structure.","code":""},{"path":"/reference/plot.ODT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"to plot an oblique decision tree — plot.ODT","text":"Lee, EK(2017) PPtreeViz: R Package Visualizing Projection Pursuit Classification Trees, Journal Statistical Software.","code":""},{"path":[]},{"path":"/reference/plot.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"to plot an oblique decision tree — plot.ODT","text":"","code":"data(iris) tree <- ODT(Species ~ ., data = iris, split = \"gini\") plot(tree)"},{"path":"/reference/plot.VarImp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Importance Plot — plot.VarImp","title":"Variable Importance Plot — plot.VarImp","text":"Dotchart variable importance measured Oblique Decision Random Forest.","code":""},{"path":"/reference/plot.VarImp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Importance Plot — plot.VarImp","text":"","code":"# S3 method for class 'VarImp' plot(x, nvar = min(30, nrow(x$varImp)), digits = NULL, main = NULL, ...)"},{"path":"/reference/plot.VarImp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Importance Plot — plot.VarImp","text":"x object class VarImp. nvar number variables show. digits Integer indicating number decimal places (round) significant digits (signif) used. main plot title. ... Arguments passed methods.","code":""},{"path":"/reference/plot.VarImp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Importance Plot — plot.VarImp","text":"horizontal axis increased error ODRF replacing variable, larger increased error important variable .","code":""},{"path":[]},{"path":"/reference/plot.VarImp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Importance Plot — plot.VarImp","text":"","code":"data(breast_cancer) set.seed(221212) train <- sample(1:569, 200) train_data <- data.frame(breast_cancer[train, -1]) forest <- ODRF(train_data[, -1], train_data[, 1], split = \"gini\",   parallel = FALSE) varimp <- VarImp(forest, train_data[, -1], train_data[, 1]) plot(varimp)"},{"path":"/reference/plot.prune.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"to plot pruned oblique decision tree — plot.prune.ODT","title":"to plot pruned oblique decision tree — plot.prune.ODT","text":"Plot error graph pruned oblique decision tree different split nodes.","code":""},{"path":"/reference/plot.prune.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"to plot pruned oblique decision tree — plot.prune.ODT","text":"","code":"# S3 method for class 'prune.ODT' plot(x, position = \"topleft\", digits = NULL, main = NULL, ...)"},{"path":"/reference/plot.prune.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"to plot pruned oblique decision tree — plot.prune.ODT","text":"x object class prune.ODT. position Position curve label, including \"topleft\" (default), \"bottomright\", \"bottom\", \"bottomleft\", \"left\", \"top\", \"topright\", \"right\" \"center\". digits Integer indicating number decimal places (round) significant digits (signif) used. main main title ... Arguments passed methods.","code":""},{"path":"/reference/plot.prune.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"to plot pruned oblique decision tree — plot.prune.ODT","text":"leftmost value horizontal axis indicates tree without pruning, rightmost value indicates data without splitting using average value predicted value.","code":""},{"path":[]},{"path":"/reference/plot.prune.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"to plot pruned oblique decision tree — plot.prune.ODT","text":"","code":"data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ])  tree <- ODT(Density ~ ., train_data, split = \"mse\") prune_tree <- prune(tree, test_data[, -1], test_data[, 1]) # Plot pruned oblique decision tree structure (default) plot(prune_tree)  # Plot the error graph of the pruned oblique decision tree. class(prune_tree) <- \"prune.ODT\" plot(prune_tree)"},{"path":"/reference/plot_ODT_depth.html","id":null,"dir":"Reference","previous_headings":"","what":"plot oblique decision tree depth — plot_ODT_depth","title":"plot oblique decision tree depth — plot_ODT_depth","text":"Draw error graph class ODT different depths.","code":""},{"path":"/reference/plot_ODT_depth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot oblique decision tree depth — plot_ODT_depth","text":"","code":"plot_ODT_depth(   formula,   data = NULL,   newdata = NULL,   split = \"gini\",   NodeRotateFun = \"RotMatPPO\",   paramList = NULL,   digits = NULL,   main = NULL,   ... )"},{"path":"/reference/plot_ODT_depth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot oblique decision tree depth — plot_ODT_depth","text":"formula Object class formula response describing model fit. data frame, taken model frame. (see model.frame) data Training data class data.frame ODT used calculate OOB error. newdata data frame matrix containing new data used calculate test error. missing, replaced data. split criterion used splitting variable. 'gini': gini impurity index (classification, default), 'entropy': information gain (classification) 'mse': mean square error (regression). NodeRotateFun Name function class character implements linear combination predictors split node. including \"RotMatPPO\": projection pursuit optimization model (PPO), see RotMatPPO (default, model=\"PPR\"). \"RotMatRF\": single feature similar Random Forest, see RotMatRF. \"RotMatRand\": random rotation, see RotMatRand. \"RotMatMake\": Users can define function, details see RotMatMake. paramList List parameters used functions NodeRotateFun. left unchanged, default values used, details see defaults. digits Integer indicating number decimal places (round) significant digits (signif) used. main main title ... Arguments passed methods.","code":""},{"path":"/reference/plot_ODT_depth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot oblique decision tree depth — plot_ODT_depth","text":"OOB error test error newdata, misclassification rate (MR) classification mean square error (MSE) regression.","code":""},{"path":[]},{"path":"/reference/plot_ODT_depth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"plot oblique decision tree depth — plot_ODT_depth","text":"","code":"data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) plot_ODT_depth(Density ~ ., train_data, test_data, split = \"mse\")"},{"path":"/reference/predict.ODRF.html","id":null,"dir":"Reference","previous_headings":"","what":"predict based on an ODRF object — predict.ODRF","title":"predict based on an ODRF object — predict.ODRF","text":"Prediction ODRF input matrix data frame.","code":""},{"path":"/reference/predict.ODRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predict based on an ODRF object — predict.ODRF","text":"","code":"# S3 method for class 'ODRF' predict(object, Xnew, type = \"response\", weight.tree = FALSE, ...)"},{"path":"/reference/predict.ODRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predict based on an ODRF object — predict.ODRF","text":"object object class ODRF, created function ODRF. Xnew n d numeric matrix (preferable) data frame. rows correspond observations columns correspond features. Note NA values data 'Xnew', replaced average value. type One response, prob tree, indicating type output: predicted values, matrix class probabilities predicted value tree. weight.tree Whether weight tree, TRUE use --bag error tree weight. (default FALSE) ... Arguments passed methods.","code":""},{"path":"/reference/predict.ODRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predict based on an ODRF object — predict.ODRF","text":"set vectors following list: response: predicted values new data. prob: matrix class probabilities (one column class one row input). object$split mse, vector tree weights returned. tree: matrix column prediction tree.","code":""},{"path":"/reference/predict.ODRF.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"predict based on an ODRF object — predict.ODRF","text":"Zhan, H., Liu, Y., & Xia, Y. (2022). Consistency Oblique Decision Tree Random Forest. arXiv preprint arXiv:2211.12653.","code":""},{"path":[]},{"path":"/reference/predict.ODRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"predict based on an ODRF object — predict.ODRF","text":"","code":"# Classification with Oblique Decision Random Forest data(seeds) set.seed(221212) train <- sample(1:209, 80) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) forest <- ODRF(varieties_of_wheat ~ ., train_data,   split = \"entropy\", parallel = FALSE,ntrees = 50 ) pred <- predict(forest, test_data[, -8], weight.tree = TRUE) # classification error (mean(pred != test_data[, 8])) #> [1] 0.03100775  # Regression with Oblique Decision Random Forest # \\donttest{ data(body_fat) set.seed(221212) train <- sample(1:252, 80) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) forest <- ODRF(Density ~ ., train_data, split = \"mse\", parallel = FALSE, ntrees = 50, TreeRandRotate=TRUE) pred <- predict(forest, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 3.66148e-05 # }"},{"path":"/reference/predict.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"making predict based on ODT objects — predict.ODT","title":"making predict based on ODT objects — predict.ODT","text":"Prediction ODT input matrix data frame.","code":""},{"path":"/reference/predict.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"making predict based on ODT objects — predict.ODT","text":"","code":"# S3 method for class 'ODT' predict(   object,   Xnew,   Xsplit = NULL,   type = c(\"pred\", \"leafnode\", \"prob\")[1],   ... )"},{"path":"/reference/predict.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"making predict based on ODT objects — predict.ODT","text":"object object class ODT, created function ODT. Xnew n d numeric matrix (preferable) data frame. rows correspond observations columns correspond features. Note NA values data 'Xnew', replaced average value. Xsplit Splitting variables used construct linear model trees. default value NULL valid split=\"linear\". type Type prediction required. Choosing \"pred\" (default) gives prediction result, choosing \"leafnode\" gives leaf node sequence number Xnew partitioned . classification tasks, including classification trees (split= \"gini\" \"entropy\") linear classification models (split= \"linear\" glmnetParList= list(family=\"binomial\" \"multinomial\")). Setting type=\"prob\" gives prediction probabilities. ... Arguments passed methods.","code":""},{"path":"/reference/predict.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"making predict based on ODT objects — predict.ODT","text":"vector following: pred: prediced response new data. leafnode: leaf node sequence number new data partitioned. prob: prediction probabilities classification tasks.","code":""},{"path":"/reference/predict.ODT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"making predict based on ODT objects — predict.ODT","text":"Zhan, H., Liu, Y., & Xia, Y. (2022). Consistency Oblique Decision Tree Random Forest. arXiv preprint arXiv:2211.12653.","code":""},{"path":[]},{"path":"/reference/predict.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"making predict based on ODT objects — predict.ODT","text":"","code":"# Classification with Oblique Decision Tree. data(seeds) set.seed(221212) train <- sample(1:209, 100) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) tree <- ODT(varieties_of_wheat ~ ., train_data, split = \"entropy\") pred <- predict(tree, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.09174312 (prob=predict(tree, test_data[, -8],type = \"prob\")) #>                 1          2         3 #>   [1,] 0.95121951 0.04878049 0.0000000 #>   [2,] 0.95121951 0.04878049 0.0000000 #>   [3,] 0.00000000 1.00000000 0.0000000 #>   [4,] 0.95121951 0.04878049 0.0000000 #>   [5,] 0.95121951 0.04878049 0.0000000 #>   [6,] 0.00000000 1.00000000 0.0000000 #>   [7,] 0.05882353 0.00000000 0.9411765 #>   [8,] 0.95121951 0.04878049 0.0000000 #>   [9,] 0.95121951 0.04878049 0.0000000 #>  [10,] 0.95121951 0.04878049 0.0000000 #>  [11,] 0.95121951 0.04878049 0.0000000 #>  [12,] 0.95121951 0.04878049 0.0000000 #>  [13,] 0.95121951 0.04878049 0.0000000 #>  [14,] 0.95121951 0.04878049 0.0000000 #>  [15,] 0.95121951 0.04878049 0.0000000 #>  [16,] 0.00000000 1.00000000 0.0000000 #>  [17,] 0.00000000 1.00000000 0.0000000 #>  [18,] 0.00000000 1.00000000 0.0000000 #>  [19,] 0.00000000 1.00000000 0.0000000 #>  [20,] 0.00000000 1.00000000 0.0000000 #>  [21,] 0.00000000 1.00000000 0.0000000 #>  [22,] 0.00000000 1.00000000 0.0000000 #>  [23,] 0.95121951 0.04878049 0.0000000 #>  [24,] 0.00000000 1.00000000 0.0000000 #>  [25,] 0.00000000 1.00000000 0.0000000 #>  [26,] 0.00000000 1.00000000 0.0000000 #>  [27,] 0.00000000 1.00000000 0.0000000 #>  [28,] 0.00000000 1.00000000 0.0000000 #>  [29,] 0.00000000 1.00000000 0.0000000 #>  [30,] 0.00000000 1.00000000 0.0000000 #>  [31,] 0.00000000 1.00000000 0.0000000 #>  [32,] 0.00000000 1.00000000 0.0000000 #>  [33,] 0.00000000 1.00000000 0.0000000 #>  [34,] 0.00000000 1.00000000 0.0000000 #>  [35,] 0.95121951 0.04878049 0.0000000 #>  [36,] 0.00000000 1.00000000 0.0000000 #>  [37,] 0.95121951 0.04878049 0.0000000 #>  [38,] 0.00000000 1.00000000 0.0000000 #>  [39,] 0.95121951 0.04878049 0.0000000 #>  [40,] 0.00000000 1.00000000 0.0000000 #>  [41,] 0.00000000 1.00000000 0.0000000 #>  [42,] 0.00000000 1.00000000 0.0000000 #>  [43,] 0.00000000 1.00000000 0.0000000 #>  [44,] 0.00000000 1.00000000 0.0000000 #>  [45,] 0.00000000 1.00000000 0.0000000 #>  [46,] 0.00000000 1.00000000 0.0000000 #>  [47,] 0.00000000 1.00000000 0.0000000 #>  [48,] 0.00000000 1.00000000 0.0000000 #>  [49,] 0.00000000 1.00000000 0.0000000 #>  [50,] 0.05882353 0.00000000 0.9411765 #>  [51,] 0.05882353 0.00000000 0.9411765 #>  [52,] 0.05882353 0.00000000 0.9411765 #>  [53,] 0.05882353 0.00000000 0.9411765 #>  [54,] 0.05882353 0.00000000 0.9411765 #>  [55,] 0.05882353 0.00000000 0.9411765 #>  [56,] 0.05882353 0.00000000 0.9411765 #>  [57,] 0.05882353 0.00000000 0.9411765 #>  [58,] 0.05882353 0.00000000 0.9411765 #>  [59,] 0.05882353 0.00000000 0.9411765 #>  [60,] 0.05882353 0.00000000 0.9411765 #>  [61,] 0.05882353 0.00000000 0.9411765 #>  [62,] 0.05882353 0.00000000 0.9411765 #>  [63,] 0.05882353 0.00000000 0.9411765 #>  [64,] 0.05882353 0.00000000 0.9411765 #>  [65,] 0.05882353 0.00000000 0.9411765 #>  [66,] 0.05882353 0.00000000 0.9411765 #>  [67,] 0.05882353 0.00000000 0.9411765 #>  [68,] 0.05882353 0.00000000 0.9411765 #>  [69,] 0.05882353 0.00000000 0.9411765 #>  [70,] 0.05882353 0.00000000 0.9411765 #>  [71,] 0.05882353 0.00000000 0.9411765 #>  [72,] 0.05882353 0.00000000 0.9411765 #>  [73,] 0.05882353 0.00000000 0.9411765 #>  [74,] 0.05882353 0.00000000 0.9411765 #>  [75,] 0.05882353 0.00000000 0.9411765 #>  [76,] 0.05882353 0.00000000 0.9411765 #>  [77,] 0.05882353 0.00000000 0.9411765 #>  [78,] 0.05882353 0.00000000 0.9411765 #>  [79,] 0.05882353 0.00000000 0.9411765 #>  [80,] 0.05882353 0.00000000 0.9411765 #>  [81,] 0.95121951 0.04878049 0.0000000 #>  [82,] 0.95121951 0.04878049 0.0000000 #>  [83,] 0.95121951 0.04878049 0.0000000 #>  [84,] 0.95121951 0.04878049 0.0000000 #>  [85,] 0.05882353 0.00000000 0.9411765 #>  [86,] 0.95121951 0.04878049 0.0000000 #>  [87,] 0.95121951 0.04878049 0.0000000 #>  [88,] 0.95121951 0.04878049 0.0000000 #>  [89,] 0.95121951 0.04878049 0.0000000 #>  [90,] 0.95121951 0.04878049 0.0000000 #>  [91,] 0.95121951 0.04878049 0.0000000 #>  [92,] 0.95121951 0.04878049 0.0000000 #>  [93,] 0.95121951 0.04878049 0.0000000 #>  [94,] 0.00000000 1.00000000 0.0000000 #>  [95,] 0.00000000 1.00000000 0.0000000 #>  [96,] 0.00000000 1.00000000 0.0000000 #>  [97,] 0.00000000 1.00000000 0.0000000 #>  [98,] 0.00000000 1.00000000 0.0000000 #>  [99,] 0.95121951 0.04878049 0.0000000 #> [100,] 0.00000000 1.00000000 0.0000000 #> [101,] 0.00000000 1.00000000 0.0000000 #> [102,] 0.05882353 0.00000000 0.9411765 #> [103,] 0.05882353 0.00000000 0.9411765 #> [104,] 0.05882353 0.00000000 0.9411765 #> [105,] 0.05882353 0.00000000 0.9411765 #> [106,] 0.05882353 0.00000000 0.9411765 #> [107,] 0.05882353 0.00000000 0.9411765 #> [108,] 0.05882353 0.00000000 0.9411765 #> [109,] 0.05882353 0.00000000 0.9411765  # Regression with Oblique Decision Tree. data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) tree <- ODT(Density ~ ., train_data, split = \"mse\") pred <- predict(tree, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 3.513866e-05  # Use \"Z\" as the splitting variable to build a linear model tree for \"X\" and \"y\". set.seed(1) n = 200 p = 10 q = 5 X = matrix(rnorm(n*p), n, p) Z = matrix(rnorm(n*q), n, q) y = (Z[,1] > 1)*(X[,1] - X[,2] + 2)  + (Z[,1] < 1)*(Z[,2] > 0)*(X[,1] + X[,2] + 0) + (Z[,1] < 1)*(Z[,2] < 0)*(X[,3] - 2) my.tree <- ODT(X=X, y=y, Xsplit=Z, split = \"linear\",                NodeRotateFun = \"RotMatRF\",MinLeaf = 10, MaxDepth = 5,                glmnetParList=list(lambda = 0.1,family = \"gaussian\")) (leafnode <- predict(my.tree, X, Xsplit=Z, type=\"leafnode\")) #>   [1] 5 5 3 4 4 5 4 4 4 4 4 4 5 4 5 3 4 4 4 3 5 4 4 5 3 4 4 5 4 5 5 4 4 4 3 3 3 #>  [38] 4 4 3 4 5 5 4 5 3 5 3 4 4 3 4 4 4 4 4 4 5 5 5 3 4 5 5 5 5 5 4 4 5 3 4 4 4 #>  [75] 5 3 4 4 5 3 5 5 5 5 4 4 5 5 5 4 4 4 4 5 4 5 3 4 5 5 4 5 3 3 5 5 3 4 4 5 5 #> [112] 3 3 5 5 5 3 5 4 5 4 4 3 4 3 4 5 5 5 3 5 5 5 5 3 4 5 5 4 4 5 5 3 4 4 5 3 5 #> [149] 5 5 5 3 4 3 5 5 4 5 4 5 4 4 5 3 4 5 3 5 3 5 3 4 5 3 4 5 4 3 5 5 4 4 4 5 4 #> [186] 5 5 5 4 5 3 3 5 4 4 3 3 5 4 3  y1 = (y>0)*1 my.tree <- ODT(X=X, y=y1, Xsplit=Z, split = \"linear\",                NodeRotateFun = \"RotMatRF\",MinLeaf = 10, MaxDepth = 5,                glmnetParList=list(family = \"binomial\")) #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground (class <- predict(my.tree, X, Xsplit=Z, type=\"pred\")) #>   [1] \"0\" \"1\" \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" \"0\" #>  [19] \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" \"0\" \"0\" \"1\" \"1\" #>  [37] \"1\" \"0\" \"0\" \"1\" \"0\" \"1\" \"1\" \"0\" \"1\" \"1\" \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" #>  [55] \"0\" \"0\" \"0\" \"0\" \"1\" \"0\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" #>  [73] \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"1\" \"0\" #>  [91] \"0\" \"1\" \"0\" \"0\" \"1\" \"1\" \"1\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" #> [109] \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"0\" \"1\" \"1\" \"0\" \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"1\" \"0\" #> [127] \"0\" \"1\" \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"1\" \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" #> [145] \"0\" \"0\" \"1\" \"0\" \"0\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" #> [163] \"1\" \"1\" \"0\" \"1\" \"1\" \"0\" \"1\" \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"1\" \"1\" \"1\" #> [181] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"1\" \"0\" \"0\" \"1\" \"1\" \"0\" #> [199] \"0\" \"1\" (prob <- predict(my.tree, X, Xsplit=Z, type=\"prob\")) #>                   0            1 #>   [1,] 7.361984e-01 2.638016e-01 #>   [2,] 5.614501e-03 9.943855e-01 #>   [3,] 5.809401e-01 4.190599e-01 #>   [4,] 9.952352e-01 4.764753e-03 #>   [5,] 9.954732e-01 4.526754e-03 #>   [6,] 6.607188e-03 9.933928e-01 #>   [7,] 9.949961e-01 5.003895e-03 #>   [8,] 9.942252e-01 5.774821e-03 #>   [9,] 8.348773e-01 1.651227e-01 #>  [10,] 9.974380e-01 2.561992e-03 #>  [11,] 9.952092e-01 4.790824e-03 #>  [12,] 9.616973e-01 3.830274e-02 #>  [13,] 9.461532e-01 5.384682e-02 #>  [14,] 9.977972e-01 2.202804e-03 #>  [15,] 2.587238e-03 9.974128e-01 #>  [16,] 4.667517e-02 9.533248e-01 #>  [17,] 9.228440e-01 7.715601e-02 #>  [18,] 9.936824e-01 6.317612e-03 #>  [19,] 9.994198e-01 5.801747e-04 #>  [20,] 3.313809e-02 9.668619e-01 #>  [21,] 9.306931e-01 6.930693e-02 #>  [22,] 9.975037e-01 2.496263e-03 #>  [23,] 9.908083e-01 9.191700e-03 #>  [24,] 9.991579e-01 8.420794e-04 #>  [25,] 2.685172e-03 9.973148e-01 #>  [26,] 7.814873e-01 2.185127e-01 #>  [27,] 9.996191e-01 3.809421e-04 #>  [28,] 9.998866e-01 1.134419e-04 #>  [29,] 9.971283e-01 2.871722e-03 #>  [30,] 1.470187e-01 8.529813e-01 #>  [31,] 2.621479e-01 7.378521e-01 #>  [32,] 9.715553e-01 2.844471e-02 #>  [33,] 9.995281e-01 4.718828e-04 #>  [34,] 9.900370e-01 9.963029e-03 #>  [35,] 2.618100e-01 7.381900e-01 #>  [36,] 1.567081e-02 9.843292e-01 #>  [37,] 4.162082e-02 9.583792e-01 #>  [38,] 9.900513e-01 9.948671e-03 #>  [39,] 9.971657e-01 2.834272e-03 #>  [40,] 8.229623e-02 9.177038e-01 #>  [41,] 9.956316e-01 4.368444e-03 #>  [42,] 7.923239e-02 9.207676e-01 #>  [43,] 6.536988e-02 9.346301e-01 #>  [44,] 9.435624e-01 5.643756e-02 #>  [45,] 2.378245e-01 7.621755e-01 #>  [46,] 9.825755e-03 9.901742e-01 #>  [47,] 7.023141e-01 2.976859e-01 #>  [48,] 1.374405e-02 9.862559e-01 #>  [49,] 9.831646e-01 1.683540e-02 #>  [50,] 9.999090e-01 9.097536e-05 #>  [51,] 2.367199e-02 9.763280e-01 #>  [52,] 9.969065e-01 3.093521e-03 #>  [53,] 9.977097e-01 2.290329e-03 #>  [54,] 9.995504e-01 4.496194e-04 #>  [55,] 9.994473e-01 5.527075e-04 #>  [56,] 9.882691e-01 1.173091e-02 #>  [57,] 9.710798e-01 2.892020e-02 #>  [58,] 8.149363e-01 1.850637e-01 #>  [59,] 8.053859e-02 9.194614e-01 #>  [60,] 8.684725e-01 1.315275e-01 #>  [61,] 7.261438e-03 9.927386e-01 #>  [62,] 5.377990e-01 4.622010e-01 #>  [63,] 2.920704e-01 7.079296e-01 #>  [64,] 6.921289e-02 9.307871e-01 #>  [65,] 4.492427e-02 9.550757e-01 #>  [66,] 2.168222e-01 7.831778e-01 #>  [67,] 9.982178e-01 1.782170e-03 #>  [68,] 9.634895e-01 3.651046e-02 #>  [69,] 8.238482e-01 1.761518e-01 #>  [70,] 3.127447e-02 9.687255e-01 #>  [71,] 8.345488e-03 9.916545e-01 #>  [72,] 9.996046e-01 3.954405e-04 #>  [73,] 9.915972e-01 8.402791e-03 #>  [74,] 9.747391e-01 2.526090e-02 #>  [75,] 9.708486e-01 2.915137e-02 #>  [76,] 1.557076e-02 9.844292e-01 #>  [77,] 9.782946e-01 2.170541e-02 #>  [78,] 9.832753e-01 1.672472e-02 #>  [79,] 9.772084e-01 2.279165e-02 #>  [80,] 9.944429e-02 9.005557e-01 #>  [81,] 6.477719e-01 3.522281e-01 #>  [82,] 8.444649e-01 1.555351e-01 #>  [83,] 8.834760e-04 9.991165e-01 #>  [84,] 9.986516e-01 1.348395e-03 #>  [85,] 4.441043e-01 5.558957e-01 #>  [86,] 3.966614e-01 6.033386e-01 #>  [87,] 2.932546e-01 7.067454e-01 #>  [88,] 1.402287e-01 8.597713e-01 #>  [89,] 1.033928e-01 8.966072e-01 #>  [90,] 9.707759e-01 2.922408e-02 #>  [91,] 9.988449e-01 1.155075e-03 #>  [92,] 4.773340e-01 5.226660e-01 #>  [93,] 9.982498e-01 1.750178e-03 #>  [94,] 9.230684e-01 7.693160e-02 #>  [95,] 3.960538e-02 9.603946e-01 #>  [96,] 1.075109e-01 8.924891e-01 #>  [97,] 4.292088e-01 5.707912e-01 #>  [98,] 9.988825e-01 1.117467e-03 #>  [99,] 9.741490e-01 2.585100e-02 #> [100,] 9.060536e-01 9.394636e-02 #> [101,] 9.877650e-01 1.223497e-02 #> [102,] 9.431436e-01 5.685640e-02 #> [103,] 7.016110e-01 2.983890e-01 #> [104,] 4.629971e-02 9.537003e-01 #> [105,] 5.768612e-02 9.423139e-01 #> [106,] 8.766523e-05 9.999123e-01 #> [107,] 3.217938e-02 9.678206e-01 #> [108,] 9.936626e-01 6.337446e-03 #> [109,] 8.997076e-01 1.002924e-01 #> [110,] 3.710176e-03 9.962898e-01 #> [111,] 2.318858e-01 7.681142e-01 #> [112,] 1.620768e-02 9.837923e-01 #> [113,] 5.783552e-03 9.942164e-01 #> [114,] 9.830564e-01 1.694358e-02 #> [115,] 6.516742e-01 3.483258e-01 #> [116,] 1.357144e-01 8.642856e-01 #> [117,] 1.862108e-02 9.813789e-01 #> [118,] 8.055981e-01 1.944019e-01 #> [119,] 9.949898e-01 5.010219e-03 #> [120,] 3.077004e-01 6.922996e-01 #> [121,] 9.976364e-01 2.363621e-03 #> [122,] 9.812554e-01 1.874458e-02 #> [123,] 5.815020e-02 9.418498e-01 #> [124,] 9.841056e-01 1.589444e-02 #> [125,] 6.198875e-02 9.380113e-01 #> [126,] 9.996539e-01 3.461128e-04 #> [127,] 8.058943e-01 1.941057e-01 #> [128,] 3.728543e-01 6.271457e-01 #> [129,] 9.885991e-01 1.140090e-02 #> [130,] 1.421219e-01 8.578781e-01 #> [131,] 7.585505e-01 2.414495e-01 #> [132,] 9.982655e-01 1.734520e-03 #> [133,] 3.094094e-01 6.905906e-01 #> [134,] 6.282343e-01 3.717657e-01 #> [135,] 3.204202e-02 9.679580e-01 #> [136,] 9.977859e-01 2.214072e-03 #> [137,] 1.475878e-01 8.524122e-01 #> [138,] 8.341364e-01 1.658636e-01 #> [139,] 9.652878e-01 3.471215e-02 #> [140,] 9.943559e-01 5.644062e-03 #> [141,] 9.751336e-01 2.486637e-02 #> [142,] 4.791231e-01 5.208769e-01 #> [143,] 1.154605e-01 8.845395e-01 #> [144,] 9.891247e-01 1.087532e-02 #> [145,] 8.665846e-01 1.334154e-01 #> [146,] 9.981867e-01 1.813264e-03 #> [147,] 3.786805e-02 9.621319e-01 #> [148,] 6.448327e-01 3.551673e-01 #> [149,] 8.328005e-01 1.671995e-01 #> [150,] 3.337966e-01 6.662034e-01 #> [151,] 3.414607e-03 9.965854e-01 #> [152,] 2.562229e-02 9.743777e-01 #> [153,] 9.989055e-01 1.094479e-03 #> [154,] 1.443614e-02 9.855639e-01 #> [155,] 4.717371e-01 5.282629e-01 #> [156,] 8.105132e-01 1.894868e-01 #> [157,] 9.999070e-01 9.303201e-05 #> [158,] 9.101187e-01 8.988126e-02 #> [159,] 9.961887e-01 3.811251e-03 #> [160,] 8.743745e-03 9.912563e-01 #> [161,] 9.991032e-01 8.967719e-04 #> [162,] 9.899880e-01 1.001200e-02 #> [163,] 2.494082e-01 7.505918e-01 #> [164,] 1.756523e-02 9.824348e-01 #> [165,] 9.969209e-01 3.079139e-03 #> [166,] 2.411388e-04 9.997589e-01 #> [167,] 2.826215e-01 7.173785e-01 #> [168,] 9.300752e-01 6.992476e-02 #> [169,] 1.151274e-02 9.884873e-01 #> [170,] 9.684758e-01 3.152420e-02 #> [171,] 2.112066e-03 9.978879e-01 #> [172,] 9.439466e-01 5.605337e-02 #> [173,] 7.797773e-01 2.202227e-01 #> [174,] 2.641557e-02 9.735844e-01 #> [175,] 9.440357e-01 5.596426e-02 #> [176,] 5.720634e-01 4.279366e-01 #> [177,] 9.670529e-01 3.294707e-02 #> [178,] 8.908167e-03 9.910918e-01 #> [179,] 7.363893e-02 9.263611e-01 #> [180,] 2.076778e-04 9.997923e-01 #> [181,] 9.993533e-01 6.466713e-04 #> [182,] 9.853368e-01 1.466319e-02 #> [183,] 7.909454e-01 2.090546e-01 #> [184,] 9.753683e-01 2.463170e-02 #> [185,] 9.722575e-01 2.774250e-02 #> [186,] 7.520352e-01 2.479648e-01 #> [187,] 8.744222e-01 1.255778e-01 #> [188,] 7.964764e-01 2.035236e-01 #> [189,] 9.777777e-01 2.222228e-02 #> [190,] 9.934724e-01 6.527559e-03 #> [191,] 4.450502e-01 5.549498e-01 #> [192,] 4.595539e-02 9.540446e-01 #> [193,] 2.344225e-01 7.655775e-01 #> [194,] 9.987123e-01 1.287668e-03 #> [195,] 9.998554e-01 1.446417e-04 #> [196,] 1.672380e-02 9.832762e-01 #> [197,] 1.280865e-01 8.719135e-01 #> [198,] 9.429641e-01 5.703590e-02 #> [199,] 9.376825e-01 6.231752e-02 #> [200,] 3.474767e-02 9.652523e-01  y2 = (y < -2.5)*1+(y>=-2.5&y<2.5)*2+(y>=2.5)*3 my.tree <- ODT(X=X, y=y2, Xsplit=Z, split = \"linear\",                NodeRotateFun = \"RotMatRF\",MinLeaf = 10, MaxDepth = 5,                glmnetParList=list(family = \"multinomial\")) #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: from glmnet C++ code (error code -94); Convergence for 94th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground #> Warning: one multinomial or binomial class has fewer than 8  observations; dangerous ground (prob <- predict(my.tree, X, Xsplit=Z, type=\"prob\")) #>                   1          2           3 #>   [1,] 2.388572e-02 0.93436323 0.041751047 #>   [2,] 1.608888e-02 0.89322784 0.090683276 #>   [3,] 1.968674e-01 0.76363998 0.039492632 #>   [4,] 8.080246e-02 0.65454931 0.264648232 #>   [5,] 3.284806e-01 0.58960657 0.081912821 #>   [6,] 1.775984e-02 0.95413717 0.028102991 #>   [7,] 1.994059e-01 0.70477239 0.095821679 #>   [8,] 1.316470e-01 0.73078318 0.137569818 #>   [9,] 2.722229e-03 0.90530439 0.091973386 #>  [10,] 6.201278e-01 0.35509434 0.024777892 #>  [11,] 8.610333e-02 0.68813425 0.225762424 #>  [12,] 2.659525e-02 0.88234489 0.091059858 #>  [13,] 1.643865e-02 0.95634464 0.027216707 #>  [14,] 5.611557e-01 0.43417561 0.004668700 #>  [15,] 1.488829e-02 0.70784884 0.277262861 #>  [16,] 3.018891e-02 0.89292271 0.076888379 #>  [17,] 1.124815e-02 0.92667329 0.062078567 #>  [18,] 1.417569e-01 0.75087634 0.107366749 #>  [19,] 2.244151e-02 0.70537071 0.272187778 #>  [20,] 1.867106e-02 0.82227434 0.159054600 #>  [21,] 1.727951e-02 0.78332712 0.199393370 #>  [22,] 5.192266e-01 0.38558012 0.095193231 #>  [23,] 1.169493e-01 0.80730597 0.075744760 #>  [24,] 3.630042e-02 0.95385961 0.009839974 #>  [25,] 2.093973e-02 0.80931388 0.169746397 #>  [26,] 1.547101e-03 0.95772574 0.040727164 #>  [27,] 8.769918e-01 0.11137833 0.011629877 #>  [28,] 2.320449e-02 0.96542431 0.011371201 #>  [29,] 6.619416e-01 0.32117100 0.016887406 #>  [30,] 2.028676e-02 0.83963733 0.140075907 #>  [31,] 1.542255e-02 0.64724426 0.337333189 #>  [32,] 3.517372e-02 0.88255681 0.082269470 #>  [33,] 8.370454e-01 0.13737423 0.025580325 #>  [34,] 2.289458e-02 0.89054709 0.086558329 #>  [35,] 2.222956e-02 0.96390830 0.013862146 #>  [36,] 2.082751e-01 0.73374450 0.057980379 #>  [37,] 2.675520e-02 0.90888760 0.064357202 #>  [38,] 1.369577e-01 0.78590663 0.077135623 #>  [39,] 3.079425e-01 0.56673802 0.125319452 #>  [40,] 2.046654e-02 0.76066143 0.218872030 #>  [41,] 2.299307e-01 0.69178344 0.078285887 #>  [42,] 9.103905e-03 0.96353726 0.027358835 #>  [43,] 1.643180e-02 0.81980763 0.163760569 #>  [44,] 6.836813e-03 0.90637229 0.086790895 #>  [45,] 1.555241e-02 0.95795211 0.026495479 #>  [46,] 2.178717e-02 0.94970183 0.028510999 #>  [47,] 2.431178e-02 0.82854727 0.147140956 #>  [48,] 2.236731e-01 0.62499733 0.151329546 #>  [49,] 6.342354e-02 0.88712517 0.049451287 #>  [50,] 9.550946e-01 0.03247400 0.012431358 #>  [51,] 1.500068e-02 0.88507425 0.099925066 #>  [52,] 4.635261e-01 0.50268405 0.033789879 #>  [53,] 6.502518e-01 0.31379907 0.035949115 #>  [54,] 9.540142e-01 0.04400966 0.001976156 #>  [55,] 7.001844e-01 0.20876676 0.091048851 #>  [56,] 4.462391e-02 0.69589972 0.259476369 #>  [57,] 4.912549e-02 0.89826664 0.052607862 #>  [58,] 3.251525e-02 0.93441341 0.033071333 #>  [59,] 2.145845e-02 0.79581130 0.182730258 #>  [60,] 2.413489e-02 0.89987348 0.075991633 #>  [61,] 6.708816e-03 0.31616554 0.677125640 #>  [62,] 2.564186e-04 0.94916680 0.050576785 #>  [63,] 1.690076e-02 0.82254904 0.160550200 #>  [64,] 1.587953e-02 0.91573954 0.068380935 #>  [65,] 1.737177e-02 0.95391474 0.028713493 #>  [66,] 2.508182e-02 0.84704510 0.127873074 #>  [67,] 1.910517e-02 0.97432200 0.006572828 #>  [68,] 1.273496e-02 0.84807809 0.139186951 #>  [69,] 1.469539e-03 0.94019148 0.058338985 #>  [70,] 7.655319e-03 0.49794198 0.494402705 #>  [71,] 5.343370e-01 0.37049555 0.095167459 #>  [72,] 9.188509e-01 0.07503477 0.006114367 #>  [73,] 2.304923e-01 0.66336384 0.106143867 #>  [74,] 2.645819e-02 0.94532714 0.028214670 #>  [75,] 2.140291e-02 0.96261899 0.015978106 #>  [76,] 8.253893e-01 0.15128827 0.023322410 #>  [77,] 4.496383e-02 0.91134460 0.043691572 #>  [78,] 5.640376e-02 0.87603876 0.067557488 #>  [79,] 1.455585e-02 0.92888411 0.056560034 #>  [80,] 1.911446e-02 0.94441825 0.036467291 #>  [81,] 1.870768e-02 0.94601832 0.035274009 #>  [82,] 2.026144e-02 0.91586789 0.063870662 #>  [83,] 1.529307e-02 0.67124244 0.313464488 #>  [84,] 2.177441e-02 0.96757353 0.010652063 #>  [85,] 1.924902e-04 0.92676294 0.073044572 #>  [86,] 1.312152e-04 0.93729653 0.062572255 #>  [87,] 1.790359e-02 0.71104411 0.271052305 #>  [88,] 2.236513e-02 0.91522432 0.062410543 #>  [89,] 1.925616e-02 0.85435221 0.126391627 #>  [90,] 2.820203e-02 0.89278063 0.079017337 #>  [91,] 2.414063e-02 0.93145949 0.044399879 #>  [92,] 2.228522e-04 0.89319804 0.106579106 #>  [93,] 4.497179e-01 0.40330097 0.146981112 #>  [94,] 2.069407e-02 0.79644547 0.182860459 #>  [95,] 1.188865e-05 0.91654426 0.083443854 #>  [96,] 1.999394e-02 0.81528115 0.164724907 #>  [97,] 1.850535e-02 0.96746240 0.014032251 #>  [98,] 6.971666e-01 0.27990976 0.022923680 #>  [99,] 1.572741e-02 0.97225978 0.012012808 #> [100,] 1.853809e-02 0.94385510 0.037606810 #> [101,] 1.111005e-01 0.85795175 0.030947737 #> [102,] 1.692422e-02 0.91877048 0.064305298 #> [103,] 6.575580e-01 0.32415573 0.018286273 #> [104,] 6.947504e-02 0.85515929 0.075365676 #> [105,] 2.004273e-02 0.94287877 0.037078506 #> [106,] 9.613456e-03 0.55383817 0.436548373 #> [107,] 2.208770e-02 0.75433143 0.223580872 #> [108,] 8.406194e-02 0.78375865 0.132179415 #> [109,] 3.787479e-03 0.90783804 0.088374483 #> [110,] 1.331548e-02 0.49296042 0.493724100 #> [111,] 1.872859e-02 0.94734836 0.033923051 #> [112,] 2.093034e-02 0.93460340 0.044466262 #> [113,] 1.258235e-02 0.67155979 0.315857866 #> [114,] 2.087253e-02 0.94652885 0.032598613 #> [115,] 2.040426e-02 0.91948320 0.060112537 #> [116,] 1.857960e-02 0.93488799 0.046532414 #> [117,] 2.501441e-01 0.70033486 0.049521069 #> [118,] 2.254647e-02 0.91744350 0.060010032 #> [119,] 1.369546e-01 0.74586014 0.117185274 #> [120,] 2.408009e-02 0.89907730 0.076842610 #> [121,] 4.139720e-01 0.54142772 0.044600316 #> [122,] 3.237234e-02 0.74495644 0.222671220 #> [123,] 4.785881e-01 0.48384414 0.037567739 #> [124,] 4.596853e-02 0.89441301 0.059618455 #> [125,] 1.347265e-01 0.78946582 0.075807682 #> [126,] 8.542548e-01 0.12052686 0.025218294 #> [127,] 2.006631e-02 0.91101213 0.068921562 #> [128,] 1.653704e-02 0.92148708 0.061975888 #> [129,] 1.735562e-02 0.95703487 0.025609510 #> [130,] 4.507960e-02 0.90657154 0.048348857 #> [131,] 2.199512e-02 0.88762273 0.090382150 #> [132,] 2.188147e-02 0.94349853 0.034620004 #> [133,] 1.082930e-02 0.90629000 0.082880697 #> [134,] 2.660600e-02 0.95807561 0.015318383 #> [135,] 1.046008e-03 0.93292876 0.066025234 #> [136,] 5.321375e-01 0.44799409 0.019868420 #> [137,] 1.837964e-02 0.93010737 0.051512986 #> [138,] 2.380481e-02 0.93022876 0.045966433 #> [139,] 2.893082e-02 0.93043524 0.040633937 #> [140,] 1.999676e-01 0.74695642 0.053075979 #> [141,] 2.414663e-02 0.96800873 0.007844636 #> [142,] 1.463685e-02 0.73536162 0.250001539 #> [143,] 2.557686e-02 0.95670346 0.017719672 #> [144,] 1.581823e-01 0.78453029 0.057287435 #> [145,] 1.048707e-02 0.96376076 0.025752172 #> [146,] 2.037155e-02 0.95330840 0.026320044 #> [147,] 9.200603e-03 0.38040042 0.610398973 #> [148,] 1.676392e-02 0.91747612 0.065759958 #> [149,] 2.889170e-02 0.94941883 0.021689467 #> [150,] 2.165344e-02 0.96731738 0.011029183 #> [151,] 1.620225e-02 0.85585555 0.127942199 #> [152,] 1.985331e-02 0.90085867 0.079288027 #> [153,] 6.782924e-01 0.29377340 0.027934200 #> [154,] 3.005734e-01 0.67173958 0.027687021 #> [155,] 2.037042e-02 0.96721165 0.012417933 #> [156,] 2.411961e-02 0.95229620 0.023584191 #> [157,] 9.508534e-01 0.03384293 0.015303677 #> [158,] 1.891513e-02 0.94918505 0.031899818 #> [159,] 4.172381e-01 0.56348511 0.019276759 #> [160,] 1.050096e-02 0.50964976 0.479849281 #> [161,] 6.501322e-01 0.29904801 0.050819823 #> [162,] 6.892109e-02 0.86917142 0.061907490 #> [163,] 1.463490e-02 0.76460794 0.220757165 #> [164,] 1.804090e-01 0.63512650 0.184464547 #> [165,] 5.369111e-01 0.44489719 0.018191714 #> [166,] 6.529609e-03 0.49778318 0.495687206 #> [167,] 1.033075e-01 0.82846836 0.068224165 #> [168,] 2.270243e-02 0.96335803 0.013939532 #> [169,] 2.502690e-02 0.89800431 0.076968790 #> [170,] 1.653749e-02 0.90706130 0.076401207 #> [171,] 3.979026e-01 0.32312216 0.278975270 #> [172,] 9.947524e-03 0.90839557 0.081656911 #> [173,] 1.803549e-02 0.86114591 0.120818595 #> [174,] 2.207595e-02 0.90082026 0.077103799 #> [175,] 1.552291e-02 0.94898149 0.035495594 #> [176,] 1.601944e-02 0.92452104 0.059459520 #> [177,] 2.695114e-02 0.86092332 0.112125544 #> [178,] 3.053887e-04 0.77145760 0.228237015 #> [179,] 1.787570e-02 0.71398715 0.268137152 #> [180,] 1.622222e-02 0.62618542 0.357592366 #> [181,] 8.235968e-01 0.16616616 0.010237070 #> [182,] 2.973629e-02 0.83357580 0.136687907 #> [183,] 1.448461e-03 0.93451390 0.064037638 #> [184,] 1.436260e-02 0.97755488 0.008082523 #> [185,] 3.053058e-02 0.86831876 0.101150655 #> [186,] 2.199849e-02 0.90898342 0.069018087 #> [187,] 1.129861e-02 0.72784598 0.260855405 #> [188,] 2.154926e-02 0.94730217 0.031148570 #> [189,] 8.685499e-02 0.84924945 0.063895557 #> [190,] 2.191810e-02 0.95460010 0.023481802 #> [191,] 1.796040e-02 0.92100029 0.061039311 #> [192,] 1.601325e-02 0.87280845 0.111178298 #> [193,] 1.801943e-02 0.95313902 0.028841552 #> [194,] 6.242149e-01 0.29440818 0.081376921 #> [195,] 9.710651e-01 0.02733155 0.001603336 #> [196,] 1.557376e-02 0.97035190 0.014074340 #> [197,] 1.611370e-02 0.50614500 0.477741295 #> [198,] 1.710175e-02 0.96554497 0.017353278 #> [199,] 1.163398e-02 0.91467703 0.073688990 #> [200,] 5.374753e-01 0.43456381 0.027960864"},{"path":"/reference/print.ODRF.html","id":null,"dir":"Reference","previous_headings":"","what":"print ODRF — print.ODRF","title":"print ODRF — print.ODRF","text":"Print contents ODRF object.","code":""},{"path":"/reference/print.ODRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print ODRF — print.ODRF","text":"","code":"# S3 method for class 'ODRF' print(x, ...)"},{"path":"/reference/print.ODRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print ODRF — print.ODRF","text":"x object class ODRF. ... Arguments passed methods.","code":""},{"path":"/reference/print.ODRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"print ODRF — print.ODRF","text":"OOB error, misclassification rate (MR) classification mean square error (MSE) regression.","code":""},{"path":[]},{"path":"/reference/print.ODRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"print ODRF — print.ODRF","text":"","code":"data(iris) forest <- ODRF(Species ~ ., data = iris, parallel = FALSE, ntrees = 50) #> Warning: You are creating a forest for classification forest #>  #> Call: #>  ODRF.formula(formula = Species ~ ., data = data, ntrees = 50,      parallel = FALSE)  #>                Type of oblique decision random forest: classification #>                                       Number of trees: 50 #>                            OOB estimate of error rate: 3.33% #> Confusion matrix: #>            setosa versicolor virginica class_error #> setosa         50          0         0  0.00000000 #> versicolor      0         47         2  0.04081624 #> virginica       0          3        48  0.05882341"},{"path":"/reference/print.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"print ODT result — print.ODT","title":"print ODT result — print.ODT","text":"Print oblique decision tree structure.","code":""},{"path":"/reference/print.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print ODT result — print.ODT","text":"","code":"# S3 method for class 'ODT' print(x, projection = FALSE, cutvalue = FALSE, verbose = TRUE, ...)"},{"path":"/reference/print.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print ODT result — print.ODT","text":"x object class ODT. projection Print projection coefficients node TRUE. cutvalue Print cutoff values node TRUE. verbose Print TRUE, output FALSE. ... Arguments passed methods.","code":""},{"path":"/reference/print.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"print ODT result — print.ODT","text":"oblique decision tree structure.","code":""},{"path":"/reference/print.ODT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"print ODT result — print.ODT","text":"Lee, EK(2017) PPtreeViz: R Package Visualizing Projection Pursuit Classification Trees, Journal Statistical Software.","code":""},{"path":[]},{"path":"/reference/print.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"print ODT result — print.ODT","text":"","code":"data(iris) tree <- ODT(Species ~ ., data = iris) #> Warning: You are creating a tree for classification tree #>  #> =============================================================  #> Oblique Classification Tree structure  #> ============================================================= #>  #> 1) root #>    node2)# proj1*X < 0.16 -> (leaf1 = setosa) #>    node3)  proj1*X >= 0.16 #>       node4)# proj2*X < 0.52 -> (leaf2 = versicolor) #>       node5)# proj2*X >= 0.52 -> (leaf3 = virginica) print(tree, projection = TRUE, cutvalue = TRUE) #>  #> =============================================================  #> Oblique Classification Tree structure  #> ============================================================= #>  #> 1) root #>    node2)# proj1*X < 0.16 -> (leaf1 = setosa) #>    node3)  proj1*X >= 0.16 #>       node4)# proj2*X < 0.52 -> (leaf2 = versicolor) #>       node5)# proj2*X >= 0.52 -> (leaf3 = virginica) #>  #> Projection coefficient in each node  #> ------------------------------------------------------------- #>       Sepal.Length Sepal.Width Petal.Length Petal.Width #> proj1       0.0000     -0.2722       0.0000      0.9622 #> proj2      -0.2008      0.0000       0.9796      0.0000 #>  #> Cutoff values of each node  #> ------------------------------------------------------------- #> CutValue1 CutValue2  #>    0.1638    0.5223"},{"path":"/reference/prune.ODRF.html","id":null,"dir":"Reference","previous_headings":"","what":"Pruning of class ODRF. — prune.ODRF","title":"Pruning of class ODRF. — prune.ODRF","text":"Prune ODRF bottom top test data based prediction error.","code":""},{"path":"/reference/prune.ODRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pruning of class ODRF. — prune.ODRF","text":"","code":"# S3 method for class 'ODRF' prune(obj, X, y, MaxDepth = 1, useOOB = TRUE, ...)"},{"path":"/reference/prune.ODRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pruning of class ODRF. — prune.ODRF","text":"obj object class ODRF. X n d numeric matrix (preferable) data frame used prune object class ODRF. y response vector length n. MaxDepth maximum depth tree pruning (Default 1). useOOB Whether use OOB pruning (Default TRUE). Note useOOB=TRUE, X y must training data ODRF. ... Optional parameters passed low level function.","code":""},{"path":"/reference/prune.ODRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pruning of class ODRF. — prune.ODRF","text":"object class ODRF prune.ODRF. ppForest result ODRF. pruneError Error test data OOB pruning tree, misclassification rate (MR) classification mean square error (MSE) regression.","code":""},{"path":[]},{"path":"/reference/prune.ODRF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pruning of class ODRF. — prune.ODRF","text":"","code":"# Classification with Oblique Decision Random Forest data(seeds) set.seed(221212) train <- sample(1:209, 80) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) forest <- ODRF(varieties_of_wheat ~ ., train_data,   split = \"entropy\", parallel = FALSE, ntrees = 50 ) prune_forest <- prune(forest, train_data[, -8], train_data[, 8]) pred <- predict(prune_forest, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.03875969 # \\donttest{ # Regression with Oblique Decision Random Forest data(body_fat) set.seed(221212) train <- sample(1:252,80) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) index <- seq(floor(nrow(train_data) / 2)) forest <- ODRF(Density ~ ., train_data[index, ], split = \"mse\", parallel = FALSE, ntrees = 50) prune_forest <- prune(forest, train_data[-index, -1], train_data[-index, 1], useOOB = FALSE) pred <- predict(prune_forest, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 5.565047e-05 # }"},{"path":"/reference/prune.ODT.html","id":null,"dir":"Reference","previous_headings":"","what":"pruning of class ODT — prune.ODT","title":"pruning of class ODT — prune.ODT","text":"Prune ODT bottom top validation data based prediction error.","code":""},{"path":"/reference/prune.ODT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"pruning of class ODT — prune.ODT","text":"","code":"# S3 method for class 'ODT' prune(obj, X, y, MaxDepth = 1, ...)"},{"path":"/reference/prune.ODT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"pruning of class ODT — prune.ODT","text":"obj object class ODT. X n d numeric matrix (preferable) data frame used prune object class ODT. y response vector length n. MaxDepth maximum depth tree pruning. (Default 1) ... Optional parameters passed low level function.","code":""},{"path":"/reference/prune.ODT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"pruning of class ODT — prune.ODT","text":"object class ODT prune.ODT. ODT result ODT. pruneError Error validation data pruning, misclassification rate (MR) classification mean square error (MSE) regression. maximum value indicates tree without pruning, minimum value (0) indicates indicates data without splitting using average value predicted value.","code":""},{"path":"/reference/prune.ODT.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"pruning of class ODT — prune.ODT","text":"leftmost value horizontal axis indicates tree without pruning, rightmost value indicates data without splitting using average value predicted value.","code":""},{"path":[]},{"path":"/reference/prune.ODT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"pruning of class ODT — prune.ODT","text":"","code":"# Classification with Oblique Decision Tree data(seeds) set.seed(221212) train <- sample(1:209, 100) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ]) index <- seq(floor(nrow(train_data) / 2)) tree <- ODT(varieties_of_wheat ~ ., train_data[index, ], split = \"entropy\") prune_tree <- prune(tree, train_data[-index, -8], train_data[-index, 8]) pred <- predict(prune_tree, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.4678899  # Regression with Oblique Decision Tree data(body_fat) set.seed(221212) train <- sample(1:252, 100) train_data <- data.frame(body_fat[train, ]) test_data <- data.frame(body_fat[-train, ]) index <- seq(floor(nrow(train_data) / 2)) tree <- ODT(Density ~ ., train_data[index, ], split = \"mse\") prune_tree <- prune(tree, train_data[-index, -1], train_data[-index, 1]) pred <- predict(prune_tree, test_data[, -1]) # estimation error mean((pred - test_data[, 1])^2) #> [1] 7.613848e-05"},{"path":"/reference/prune.html","id":null,"dir":"Reference","previous_headings":"","what":"prune ODT or ODRF — prune","title":"prune ODT or ODRF — prune","text":"Prune ODT ODRF bottom top validation data based prediction error, prune S3 method class ODT ODRF.","code":""},{"path":"/reference/prune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"prune ODT or ODRF — prune","text":"","code":"prune(obj, ...)"},{"path":"/reference/prune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prune ODT or ODRF — prune","text":"obj object class ODT ODRF. ... parameters related class obj, see ODT ODRF.","code":""},{"path":"/reference/prune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"prune ODT or ODRF — prune","text":"object class ODT prune.ODT.","code":""},{"path":[]},{"path":"/reference/seeds.html","id":null,"dir":"Reference","previous_headings":"","what":"seeds Data Set — seeds","title":"seeds Data Set — seeds","text":"Measurements geometrical properties kernels belonging three different varieties wheat. soft X-ray technique GRAINS package used construct seven, real-valued attributes.","code":""},{"path":"/reference/seeds.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"seeds Data Set — seeds","text":"data frame 209 rows 7 covariate variables 1 response variable.","code":""},{"path":"/reference/seeds.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"seeds Data Set — seeds","text":"https://archive.ics.uci.edu/ml/datasets/seeds","code":""},{"path":"/reference/seeds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"seeds Data Set — seeds","text":"variables listed , left right, : area perimeter P compactness C = 4piA/P^2 length kernel width kernel asymmetry coefficient length kernel groove varieties wheat (1, 2, 3 Kama, Rosa Canadian respectively)","code":""},{"path":"/reference/seeds.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"seeds Data Set — seeds","text":"M. Charytanowicz, J. Niewczas, P. Kulczycki, P.. Kowalski, S. Lukasik, S. Zak, 'Complete Gradient Clustering Algorithm Features Analysis X-ray Images', : Information Technologies Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24.","code":""},{"path":[]},{"path":"/reference/seeds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"seeds Data Set — seeds","text":"","code":"data(seeds) set.seed(221212) train <- sample(1:209, 80) train_data <- data.frame(seeds[train, ]) test_data <- data.frame(seeds[-train, ])  forest <- ODRF(varieties_of_wheat ~ ., train_data,   split = \"gini\", parallel = FALSE, ntrees = 50 ) pred <- predict(forest, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.03100775  tree <- ODT(varieties_of_wheat ~ ., train_data, split = \"gini\") pred <- predict(tree, test_data[, -8]) # classification error (mean(pred != test_data[, 8])) #> [1] 0.0620155"},{"path":"/news/index.html","id":"odrf-development-version","dir":"Changelog","previous_headings":"","what":"ODRF (development version)","title":"ODRF (development version)","text":"Added linear model tree. Specifically, use parameter Xsplit splitting variable ODT fit linear model split using function “gmlnet”. corresponding parameter split=“linear”. Added ensemble ODT-based boosting trees，denoted ODBT. Changed parameter “leafnode” “type” function predict.ODT(), classification tasks, added category probability output. Optimized known issues.","code":""},{"path":"/news/index.html","id":"odrf-004","dir":"Changelog","previous_headings":"","what":"ODRF 0.0.4","title":"ODRF 0.0.4","text":"CRAN release: 2023-05-28 Fixed function VarImp(), adding method measuring importance variables node purity, now VarImp() can used class ODT ODRF. argument “Xcat ! = 0”, .e., category variable predictor X transformed one--K encode. however argument “NodeRotateFun=‘RotMatRF’ (‘RotMatRand’)“ run error, now fixed . Added predicted values training data class ODT ODRF. Fixed issue related function predict.ODRF() argument “weight.tree = TRUE”. Optimized known issues.","code":""},{"path":"/news/index.html","id":"odrf-003","dir":"Changelog","previous_headings":"","what":"ODRF 0.0.3","title":"ODRF 0.0.3","text":"CRAN release: 2023-03-16 function predicate.ODT() runs error ODT split (depth=1), fixed bug. fixed function predict.ODRF() arguments numOOB weight.tree related issues. fixed functions plot.ODT(), VarImp() plot.VarImp(). fixed argument ‘lambda’ functions ODT() ODRF().","code":""},{"path":"/news/index.html","id":"odrf-002","dir":"Changelog","previous_headings":"","what":"ODRF 0.0.2","title":"ODRF 0.0.2","text":"CRAN release: 2023-02-28 now explained CART Random Forest description text. changed Date field recent date. now exported functions RandRot() defaults(), longer need ODRF::: removed par plot.VarImp() added .exit plot.prune.ODT(), checked code make sure change user’s options, including par working directory. removed random seed number functions ODRF(), poune.ODRF(), online.ODRF() plot_ODT_depth().","code":""},{"path":"/news/index.html","id":"odrf-001","dir":"Changelog","previous_headings":"","what":"ODRF 0.0.1","title":"ODRF 0.0.1","text":"Added NEWS.md file track changes package. first fully-functioning version package. currently ERRORs, WARNINGs, NOTEs devtools::check().","code":""}]
